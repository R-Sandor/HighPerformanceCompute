{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25839c2-9d92-4789-9dd7-353952d23077",
   "metadata": {},
   "source": [
    "# ReadMe\n",
    "The objective of this project is to read the National Vulnerability Database's (NVD) of Common exposures and vulnerabilites (CVES) and create a tool \n",
    "that is used as a part of the data pipeline to determine how similar CVE's are to one another. \n",
    "\n",
    "The first goal to such a pipeline is to clean the description data, tokenize the data. The second goal is tokenize and read the data that will be used as a training label. \n",
    "\n",
    "# Requirements\n",
    "Required packages:\n",
    "- pyspark\n",
    "- numpy\n",
    "\n",
    "TF-IDF use cases:\n",
    "- LDA\n",
    "- Similarity with\n",
    "1. **Feature Selection**:\n",
    "   - Examine the top TF-IDF terms to identify important features. You can select a subset of these features for further analysis or modeling.\n",
    "   - Consider removing low-TF-IDF terms (common words) that might not contribute significantly to your task.\n",
    "\n",
    "2. **Clustering and Topic Modeling**:\n",
    "   - Apply clustering algorithms (e.g., K-means, DBSCAN) to group similar documents based on their TF-IDF vectors.\n",
    "   - Explore topic modeling techniques (e.g., Latent Dirichlet Allocation, Non-Negative Matrix Factorization) to discover latent topics within your corpus.\n",
    "\n",
    "3. **Document Similarity**:\n",
    "   - Calculate cosine similarity between TF-IDF vectors of different documents. This helps identify similar documents.\n",
    "   - Use similarity scores to recommend related articles, products, or content.\n",
    "\n",
    "4. **Classification and Sentiment Analysis**:\n",
    "   - Train classifiers (e.g., SVM, Random Forest) using TF-IDF features as input. This is useful for tasks like sentiment analysis, spam detection, or document categorization.\n",
    "   - Convert text data into TF-IDF vectors and use them as features for machine learning models.\n",
    "\n",
    "5. **Search and Information Retrieval**:\n",
    "   - Build an inverted index using TF-IDF vectors to create an efficient search engine.\n",
    "   - Retrieve relevant documents based on user queries by ranking them using their TF-IDF scores.\n",
    "\n",
    "6. **Visualizations**:\n",
    "   - Visualize TF-IDF scores using word clouds, scatter plots, or bar charts to gain insights into term importance.\n",
    "   - Plot the distribution of TF-IDF values across the entire dataset.\n",
    "\n",
    "7. **Optimize Hyperparameters**:\n",
    "   - Experiment with different parameters (e.g., n-grams, stop words, max features) in your TF-IDF vectorization process.\n",
    "   - Use cross-validation to find optimal settings.\n",
    "\n",
    "TODO: Find sources to back up these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec292ec-4971-401b-bb8a-df0d79a467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, re\n",
    "import pyspark\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  col, count, countDistinct, concat_ws, explode, expr, lit, udf, split, sum\n",
    "from pyspark.sql.types import DoubleType, FloatType, StringType, ArrayType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import VectorUDT, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abf5b21-76a6-45a8-862b-ab0e82b43e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset if it doesn't already exist. \n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True) \n",
    "fileUrls = [\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2003.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2004.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2005.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2006.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2007.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2008.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2009.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2010.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2011.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-recent.json.zip'\n",
    "    ]\n",
    "\n",
    "# Iterate through each URL\n",
    "for url in fileUrls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    outputfile = os.path.join(data_dir, filename)\n",
    "    checkfile = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(checkfile):\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, outputfile)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "        # Extract the file\n",
    "        with zipfile.ZipFile(outputfile, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "        # Delete the original zip file\n",
    "        os.remove(outputfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a35d5c-291b-44b8-b51d-71ad35bf0f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 07:58:04 WARN Utils: Your hostname, sandbox resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface eth0)\n",
      "24/03/19 07:58:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 07:58:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/19 07:58:10 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (192.168.56.10 executor 1): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 3516924559342767982, local class serialVersionUID = 823754013007382808\n",
      "\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/03/19 07:58:10 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (192.168.56.14 executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 3516924559342767982, local class serialVersionUID = 823754013007382808\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)\n\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.$anonfun$infer$5(JsonDataSource.scala:167)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.infer(JsonDataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 3516924559342767982, local class serialVersionUID = 823754013007382808\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark://hpd-master:7077\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# read the data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m cves \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/nvdcve-1.1-2020.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Manipulate the data to be more usable \u001b[39;00m\n\u001b[1;32m     16\u001b[0m exploded \u001b[38;5;241m=\u001b[39m cves\u001b[38;5;241m.\u001b[39mselect(explode(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCVE_Items\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcves\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/sql/readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (192.168.56.14 executor 0): java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 3516924559342767982, local class serialVersionUID = 823754013007382808\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)\n\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.$anonfun$infer$5(JsonDataSource.scala:167)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.infer(JsonDataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.InvalidClassException: org.apache.spark.rdd.RDD; local class incompatible: stream classdesc serialVersionUID = 3516924559342767982, local class serialVersionUID = 823754013007382808\n\tat java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:597)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2051)\n\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1898)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2224)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:129)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"voltcve\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.default.parallelism\", 8)\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "       .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "# read the data\n",
    "cves = spark.read.option(\"multiline\", \"true\").json(\"data/nvdcve-1.1-2020.json\")\n",
    "\n",
    "# Manipulate the data to be more usable \n",
    "exploded = cves.select(explode(col(\"CVE_Items\")).alias(\"cves\"))\n",
    "\n",
    "\n",
    "# Now 'exploded' contains individual rows for each 'CVE_Item'\n",
    "# Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "# cpe:<cpe_version>:<part>:<vendor>:<product>:<version>:<update>:<edition>:<language>:<sw_edition>:<target_sw>:<target_hw>:<other>\n",
    "cpe_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"), explode(col(\"cves.configurations.nodes.cpe_match\")).alias(\"cpe\"))\n",
    "\n",
    "# Take just the first row to train. The CPE23Uri will contain a lot of repeated data\n",
    "# As each version is given a row.\n",
    "cpe_df = cpe_df.select(col(\"id\"), col(\"cpe\")[0].alias(\"cpe\"))\n",
    "cpe_df = cpe_df.select(\"id\", split(cpe_df.cpe.cpe23uri,\":\",-1)[4].alias(\"label\"))\n",
    "\n",
    "cpe_df.show(truncate=200)\n",
    "cpe_df.printSchema()\n",
    "\n",
    "descr_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"),\n",
    "          col(\"cves.cve.description.description_data.value\").alias(\"description\"));\n",
    "\n",
    "descr_df = descr_df.withColumn(\"description_single\", concat_ws(\" \", descr_df[\"description\"]))\n",
    "\n",
    "\n",
    "doc_count = descr_df.selectExpr(\"count(distinct id)\").first()[0]\n",
    "print(\"Number of docs: {}\".format(doc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5f30d-ca76-4915-b2a7-fe3fa3c6e0b0",
   "metadata": {},
   "source": [
    "## Questions from the data:\n",
    "- Should CPE be trained on the same data with multiple labels?\n",
    "- How much data is enough to adequately train a word2vec model?\n",
    "\n",
    "## Tokenization: \n",
    "### Applying the right tokenization methods:\n",
    "The dataset contains a lot of data the ordinary tokenization may not apply. For example, file names contain puncutation and can leave the token meaningless. \n",
    "An effort was made to preserve all meaninful punction that would describe software or hardware configurations, while removing stop words, and ordinary english punctation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850909ac-b461-45b6-8d55-327f2d71609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def string_cleaner(input_str):\n",
    "    # 1. Replace all \".\" or ':' followed by whitespace with an empty string.\n",
    "    # a. Remove ending periods.\n",
    "    # 2. Remove trademark, rights.\n",
    "    # 3. Grab cotent in parentheses only.\n",
    "    # 4. Remove some punctuation.\n",
    "    cleaned_text = re.sub(r\"[.:,]+\\s+\", \" \", input_str)\n",
    "    # Remove trailing periods\n",
    "    cleaned_text = re.sub(r\"\\.$\", \"\", cleaned_text)\n",
    "    # Remove apostrophes, (TM), (R), parentheses, and double quotes\n",
    "    cleaned_text = re.sub(r\"\\'|\\(TM\\)|\\(R\\)|\\(|\\)|\\\"\", \"\", cleaned_text)\n",
    "    # Convert to lowercase and strip leading/trailing spaces\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "clean_tokens = descr_df.withColumn(\"token\", split(string_cleaner(col(\"description_single\")), \" \" ))\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"token\", outputCol=\"cleanToken\")\n",
    "clean_tokens = stop_words_remover.transform(clean_tokens)\n",
    "clean_tokens = clean_tokens.select(\"id\", explode(\"cleantoken\").alias(\"token\"))\n",
    "clean_tokens = clean_tokens.filter(col(\"token\").isNotNull())\n",
    "\n",
    "# # Show the resulting DataFrame\n",
    "clean_tokens.show(truncate=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f8695-9403-4b47-a7ab-0d39a08edafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tokens, doc_count):\n",
    "    allTokensForId = tokens.groupBy(\"id\").agg(count(\"id\").alias(\"allTokensForId\"))\n",
    "\n",
    "    tfds = tokens.groupBy(\"id\", \"token\").agg(count(\"id\").alias(\"rawtf\"))\n",
    "    dfds = tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\"))\n",
    "\n",
    "    # Join the two DataFrames on 'id'\n",
    "    merged_df = tfds.join(allTokensForId, on=\"id\")\n",
    "\n",
    "    # Calculate the ratio of rawtf to allTokensForId\n",
    "    tfds = merged_df.withColumn(\"tf\", col(\"rawtf\") / col(\"allTokensForId\"))\n",
    "    \n",
    "\n",
    "    merged_df.show()\n",
    "\n",
    "    # Define the UDF for idf calculation\n",
    "    spark.udf.register(\"calcidfudf\", lambda df: calcidf(doc_count, df), DoubleType())\n",
    "\n",
    "    # Calculate idf and add it as a new column \"idf\"\n",
    "    tokens_idf = dfds.withColumn(\"idf\", expr(\"calcidfudf(df)\"))\n",
    "\n",
    "    # Show the resulting dataframe\n",
    "    tfidfds = tokens_idf.join(tfds, \"token\", \"left\") \\\n",
    "        .withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "\n",
    "    return tfidfds\n",
    "\n",
    "def calcidf(doc_count, df):\n",
    "    # Calculate the tf-idf using natural log\n",
    "    return math.log((doc_count + 1.0) / (df + 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ff4a5-c2c5-4cee-99bb-e609153b7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf(clean_tokens, doc_count)\n",
    "\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402679d-b95e-415f-93b5-d72d01fc264b",
   "metadata": {},
   "source": [
    "Already calculated the TF-IDF scores for the documents. Now, let’s create a TF-IDF vector from these scores.\n",
    "\n",
    "Assuming you have a DataFrame named tfidf_vector with columns ‘id’, ‘token’, ‘tf_idf’, and ‘allTokensForId’, you can proceed as follows:\n",
    "\n",
    "    Group TF-IDF Scores by Document ID:\n",
    "        Group the DataFrame by the ‘id’ column and aggregate the ‘tf_idf’ values into a list for each document.\n",
    "        This will give you a list of TF-IDF scores for each document.\n",
    "\n",
    "    Create a Sparse Vector Representation:\n",
    "        Use the SparseVector class from PySpark to create a sparse vector representation for each document.\n",
    "        The vector will have dimensions equal to the total number of unique tokens (terms) in your dataset.\n",
    "        For each document, set the value at the index corresponding to the token to its corresponding TF-IDF score.\n",
    "\n",
    "    Assemble the Sparse Vectors:\n",
    "        Assemble the sparse vectors into a single column using the VectorAssembler.\n",
    "        This will give you a new DataFrame with a column containing the TF-IDF vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7131c-23d2-4ba2-a8fb-f579cb838370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "# Create Sparse Vectors\n",
    "def create_sparse_vector(tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens  # Initialize all values to 0.0\n",
    "    for i, tfidf_score in enumerate(tfidf_list):\n",
    "        values[i] = tfidf_score\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "num_tokens = len(tfidf_df.select('token').distinct().collect())\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('tfidf_list', lit(num_tokens)))\n",
    "\n",
    "# Assemble the Sparse Vectors\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_tfidf_vector.select('id', 'tfidf_features').show(truncate= 400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaf3fd-3797-4f44-9043-bb762e1ac213",
   "metadata": {},
   "source": [
    "Now that you have the TF-IDF vectors for the documents, here are some useful things to do with the results:\n",
    "\n",
    "    Document Similarity:\n",
    "        Calculate the similarity between documents using cosine similarity or other distance metrics. The closer the vectors, the more similar the documents.\n",
    "        For example, you can find similar documents to a given query document by comparing their TF-IDF vectors.\n",
    "\n",
    "    Topic Modeling:\n",
    "        Apply topic modeling techniques (such as Latent Dirichlet Allocation or Non-Negative Matrix Factorization) to discover underlying topics in your corpus.\n",
    "        Use the TF-IDF vectors as input for these models.\n",
    "\n",
    "    Classification and Clustering:\n",
    "        Train machine learning models (e.g., SVM, Random Forest, or k-means) using the TF-IDF vectors as features.\n",
    "        Classify documents into predefined categories or cluster similar documents together.\n",
    "\n",
    "    Keyword Extraction:\n",
    "        Identify important keywords or phrases within each document based on their TF-IDF scores.\n",
    "        Higher TF-IDF scores indicate more significant terms.\n",
    "\n",
    "    Search and Retrieval:\n",
    "        Use the TF-IDF vectors to build an efficient search index for your documents.\n",
    "        Given a query, retrieve relevant documents based on their similarity to the query.\n",
    "\n",
    "    Visualizations:\n",
    "        Visualize the TF-IDF vectors in lower dimensions using techniques like t-SNE or PCA.\n",
    "        Explore the distribution of documents in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097ca7a-c628-4d7f-a655-76d14c28d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize the vectors\n",
    "normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"normFeatures\")\n",
    "data = normalizer.transform(final_tfidf_vector)\n",
    "\n",
    "# Get the first document's features\n",
    "first_doc_features = data.first().normFeatures\n",
    "\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "# def cosine_similarity(v):\n",
    "#     return float(first_doc_features.dot(v)) / (first_doc_features.norm(2) * v.norm(2))\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "# def cosine_similarity(v):\n",
    "#     dot_product = float(first_doc_features.dot(v))\n",
    "#     norm_product = math.sqrt(sum([i**2 for i in first_doc_features])) * math.sqrt(sum([i**2 for i in v]))\n",
    "#     return dot_product / norm_product\n",
    "\n",
    "# def cosine_similarity(v):\n",
    "#     dot_product = float(first_doc_features.dot(v))\n",
    "#     norm_product = first_doc_features.norm(2) * v.norm(2)\n",
    "#     return dot_product / norm_product\n",
    "\n",
    "def cosine_similarity(v):\n",
    "    dot_product = float(first_doc_features.dot(v))\n",
    "    norm_product = float(first_doc_features.norm(2)) * float(v.norm(2))\n",
    "    return float(dot_product) / float(norm_product)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "\n",
    "# Compute the cosine similarity and add it as a new column\n",
    "data = data.withColumn(\"cosine_sim\", cosine_similarity_udf(col(\"normFeatures\")))\n",
    "\n",
    "# Show the top 5 documents\n",
    "#data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f64e0d-afdb-464d-a680-97665b63017b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 5\n",
    "data.orderBy(col('cosine_sim').desc()).select('id', 'cosine_sim').limit(N).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77fb5b-76ef-45c3-9d0c-7ea7e2b210d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102ee23-5906-4781-95a5-f514033ecd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f82bcc-458e-4599-9b31-bb781f28bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a829a-d5c9-4d6a-be16-8903b6521971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
