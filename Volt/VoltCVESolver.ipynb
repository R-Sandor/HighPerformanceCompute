{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25839c2-9d92-4789-9dd7-353952d23077",
   "metadata": {},
   "source": [
    "# Volt\n",
    "# Summary \n",
    "In this project the National Vulnerability Database's (NVD) of Common exposures and vulnerabilites (CVES) is levarged\n",
    "to create a tool that is used as a part of the data pipeline to determine how similar CVE's are to one another. \n",
    "\n",
    "The first goal to such a pipeline is to clean the description data, tokenize the data. The second goal is tokenize and read the data that will be used as a training label. \n",
    "## Project Objective: \n",
    "### Create a Pipeline that Can Support Document Similarity and Search\n",
    "\n",
    "\n",
    "# Requirements\n",
    "Required packages:\n",
    "- pyspark\n",
    "- numpy\n",
    "\n",
    "TF-IDF use cases:\n",
    "- LDA\n",
    "- Similarity with\n",
    "1. **Feature Selection**:\n",
    "   - Examine the top TF-IDF terms to identify important features. You can select a subset of these features for further analysis or modeling.\n",
    "   - Consider removing low-TF-IDF terms (common words) that might not contribute significantly to your task.\n",
    "\n",
    "2. **Clustering and Topic Modeling**:\n",
    "   - Apply clustering algorithms (e.g., K-means, DBSCAN) to group similar documents based on their TF-IDF vectors.\n",
    "   - Explore topic modeling techniques (e.g., Latent Dirichlet Allocation, Non-Negative Matrix Factorization) to discover latent topics within your corpus.\n",
    "\n",
    "3. **Document Similarity**:\n",
    "   - Calculate cosine similarity between TF-IDF vectors of different documents. This helps identify similar documents.\n",
    "   - Use similarity scores to recommend related articles, products, or content.\n",
    "\n",
    "4. **Classification and Sentiment Analysis**:\n",
    "   - Train classifiers (e.g., SVM, Random Forest) using TF-IDF features as input. This is useful for tasks like sentiment analysis, spam detection, or document categorization.\n",
    "   - Convert text data into TF-IDF vectors and use them as features for machine learning models.\n",
    "\n",
    "5. **Search and Information Retrieval**:\n",
    "   - Build an inverted index using TF-IDF vectors to create an efficient search engine.\n",
    "   - Retrieve relevant documents based on user queries by ranking them using their TF-IDF scores.\n",
    "\n",
    "6. **Visualizations**:\n",
    "   - Visualize TF-IDF scores using word clouds, scatter plots, or bar charts to gain insights into term importance.\n",
    "   - Plot the distribution of TF-IDF values across the entire dataset.\n",
    "\n",
    "7. **Optimize Hyperparameters**:\n",
    "   - Experiment with different parameters (e.g., n-grams, stop words, max features) in your TF-IDF vectorization process.\n",
    "   - Use cross-validation to find optimal settings.\n",
    "\n",
    "TODO: Find sources to back up these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec292ec-4971-401b-bb8a-df0d79a467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, re\n",
    "import pyspark\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, countDistinct, concat_ws, desc, explode, expr, lit, udf, split, sum\n",
    "from pyspark.sql.types import DoubleType, FloatType, StringType, ArrayType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import VectorUDT, SparseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d229f2-1456-48f6-933b-7aa286239fb8",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Handle downloading the data if it doesn't already exist for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abf5b21-76a6-45a8-862b-ab0e82b43e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset if it doesn't already exist. \n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True) \n",
    "fileUrls = [\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2003.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2004.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2005.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2006.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2007.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2008.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2009.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2010.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2011.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-recent.json.zip'\n",
    "    ]\n",
    "\n",
    "# Iterate through each URL\n",
    "for url in fileUrls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    outputfile = os.path.join(data_dir, filename)\n",
    "    checkfile = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(checkfile):\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, outputfile)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "        # Extract the file\n",
    "        with zipfile.ZipFile(outputfile, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "        # Delete the original zip file\n",
    "        os.remove(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0844e87-acd6-484f-b351-063213f0be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 10:53:15 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.0.45 instead (on interface eno0)\n",
      "24/03/22 10:53:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/22 10:53:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CVE_Items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- configurations: struct (nullable = true)\n",
      " |    |    |    |-- CVE_data_version: string (nullable = true)\n",
      " |    |    |    |-- nodes: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- cpe_match: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- cpe23Uri: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- cpe_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionEndExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionEndIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionStartExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionStartIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- vulnerable: boolean (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- operator: string (nullable = true)\n",
      " |    |    |    |    |    |-- cpe_match: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- cpe23Uri: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- cpe_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- versionEndExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionEndIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionStartExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionStartIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- vulnerable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- operator: string (nullable = true)\n",
      " |    |    |-- cve: struct (nullable = true)\n",
      " |    |    |    |-- CVE_data_meta: struct (nullable = true)\n",
      " |    |    |    |    |-- ASSIGNER: string (nullable = true)\n",
      " |    |    |    |    |-- ID: string (nullable = true)\n",
      " |    |    |    |-- data_format: string (nullable = true)\n",
      " |    |    |    |-- data_type: string (nullable = true)\n",
      " |    |    |    |-- data_version: string (nullable = true)\n",
      " |    |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |    |-- description_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- problemtype: struct (nullable = true)\n",
      " |    |    |    |    |-- problemtype_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- description: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- references: struct (nullable = true)\n",
      " |    |    |    |    |-- reference_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- refsource: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- impact: struct (nullable = true)\n",
      " |    |    |    |-- baseMetricV2: struct (nullable = true)\n",
      " |    |    |    |    |-- acInsufInfo: boolean (nullable = true)\n",
      " |    |    |    |    |-- cvssV2: struct (nullable = true)\n",
      " |    |    |    |    |    |-- accessComplexity: string (nullable = true)\n",
      " |    |    |    |    |    |-- accessVector: string (nullable = true)\n",
      " |    |    |    |    |    |-- authentication: string (nullable = true)\n",
      " |    |    |    |    |    |-- availabilityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- baseScore: double (nullable = true)\n",
      " |    |    |    |    |    |-- confidentialityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- integrityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- vectorString: string (nullable = true)\n",
      " |    |    |    |    |    |-- version: string (nullable = true)\n",
      " |    |    |    |    |-- exploitabilityScore: double (nullable = true)\n",
      " |    |    |    |    |-- impactScore: double (nullable = true)\n",
      " |    |    |    |    |-- obtainAllPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- obtainOtherPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- obtainUserPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- severity: string (nullable = true)\n",
      " |    |    |    |    |-- userInteractionRequired: boolean (nullable = true)\n",
      " |    |    |    |-- baseMetricV3: struct (nullable = true)\n",
      " |    |    |    |    |-- cvssV3: struct (nullable = true)\n",
      " |    |    |    |    |    |-- attackComplexity: string (nullable = true)\n",
      " |    |    |    |    |    |-- attackVector: string (nullable = true)\n",
      " |    |    |    |    |    |-- availabilityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- baseScore: double (nullable = true)\n",
      " |    |    |    |    |    |-- baseSeverity: string (nullable = true)\n",
      " |    |    |    |    |    |-- confidentialityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- integrityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- privilegesRequired: string (nullable = true)\n",
      " |    |    |    |    |    |-- scope: string (nullable = true)\n",
      " |    |    |    |    |    |-- userInteraction: string (nullable = true)\n",
      " |    |    |    |    |    |-- vectorString: string (nullable = true)\n",
      " |    |    |    |    |    |-- version: string (nullable = true)\n",
      " |    |    |    |    |-- exploitabilityScore: double (nullable = true)\n",
      " |    |    |    |    |-- impactScore: double (nullable = true)\n",
      " |    |    |-- lastModifiedDate: string (nullable = true)\n",
      " |    |    |-- publishedDate: string (nullable = true)\n",
      " |-- CVE_data_format: string (nullable = true)\n",
      " |-- CVE_data_numberOfCVEs: string (nullable = true)\n",
      " |-- CVE_data_timestamp: string (nullable = true)\n",
      " |-- CVE_data_type: string (nullable = true)\n",
      " |-- CVE_data_version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"voltcve\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.default.parallelism\", 8)\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "       .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "# read the data\n",
    "cves = spark.read.option(\"multiline\", \"true\").json(\"data/nvdcve-1.1-2020.json\")\n",
    "\n",
    "# Manipulate the data to be more usable \n",
    "cves.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6172a-4a1e-46f4-8920-697f3793ff3b",
   "metadata": {},
   "source": [
    "# Complex JSON\n",
    "Look at the Schema above, the data is complex and Spark can read the data into a DataFrame. The DataFrame being equvielent to Spark SQL. \n",
    "\n",
    "From the complex schema the data needs to be turned into a more usable format. \n",
    "\n",
    "## Section Objective:\n",
    "- Get Ids\n",
    "- Description Data\n",
    "- Remove unnecessary nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32967fa-907f-4385-86a4-07a81cee8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = cves.select(explode(col(\"CVE_Items\")).alias(\"cves\"))\n",
    "\n",
    "descr_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"),\n",
    "          col(\"cves.cve.description.description_data.value\").alias(\"description\"));\n",
    "\n",
    "descr_df = descr_df.withColumn(\"description_single\", concat_ws(\" \", descr_df[\"description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e1de82-2fce-480c-b534-a40392dc6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 20453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_count = descr_df.selectExpr(\"count(distinct id)\").first()[0]\n",
    "print(\"Number of docs: {}\".format(doc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a2b2a-a08d-4ae5-ac04-68e27baadf8a",
   "metadata": {},
   "source": [
    "# CVE Product Info\n",
    "Below the goal is to get useful information about the CVEs. For example, the type of tool or software that had a vulnerability.\n",
    "- Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "- If it is not used in the application the data may be used in other applications the data later.\n",
    "\n",
    "\n",
    "Example of CPEs: \\\n",
    "cpe:2.3:a:ntp:ntp:4.2.8:p3:*:*:*:*:*:* \\\n",
    "cpe:2.3:o:microsoft:windows_7:-:sp2:*:*:*:*:*:* \\\n",
    "cpe:2.3:a:microsoft:internet_explorer:8.0.6001:beta:*:*:*:*:*:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc9d497-4eab-40fc-a888-62af78b67b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|           id|product|\n",
      "+-------------+-------+\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0003|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0009|android|\n",
      "|CVE-2020-0010|android|\n",
      "|CVE-2020-0011|android|\n",
      "|CVE-2020-0012|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0016|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "+-------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now 'exploded' contains individual rows for each 'CVE_Item'\n",
    "# Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "# cpe:<cpe_version>:<part>:<vendor>:<product>:<version>:<update>:<edition>:<language>:<sw_edition>:<target_sw>:<target_hw>:<other>\n",
    "cpe_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"), explode(col(\"cves.configurations.nodes.cpe_match\")[0]).alias(\"cpe\"))\n",
    "\n",
    "cpe_df = cpe_df.select(col(\"id\"), col(\"cpe\").alias(\"cpe\"))\n",
    "cpe_df = cpe_df.select(\"id\", split(cpe_df.cpe.cpe23uri,\":\",-1)[4].alias(\"product\"))\n",
    "\n",
    "# Sample of the data - schema/show\n",
    "cpe_df.printSchema()\n",
    "cpe_df.show(n=50, truncate=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5f30d-ca76-4915-b2a7-fe3fa3c6e0b0",
   "metadata": {},
   "source": [
    "## Uses from the cpe data:\n",
    "- CPE could be a training label.\n",
    "- CPE used in clustering.\n",
    "- Document filtering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a058d1-e741-479e-9e49-4fedc15f71f8",
   "metadata": {},
   "source": [
    "# Tokenization: \n",
    "## Applying the right tokenization methods:\n",
    "The dataset contains a lot of data the ordinary tokenization may not apply. For example, file names contain puncutation and can leave the token meaningless. \n",
    "An effort was made to preserve all meaninful punction that would describe software or hardware configurations, while removing stop words, and ordinary english punctation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "850909ac-b461-45b6-8d55-327f2d71609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 795:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|           id|               token|\n",
      "+-------------+--------------------+\n",
      "|CVE-2020-0001|getprocessrecordl...|\n",
      "|CVE-2020-0001|activitymanagerse...|\n",
      "|CVE-2020-0001|            isolated|\n",
      "|CVE-2020-0001|                apps|\n",
      "|CVE-2020-0001|             handled|\n",
      "|CVE-2020-0001|           correctly|\n",
      "|CVE-2020-0001|                lead|\n",
      "|CVE-2020-0001|               local|\n",
      "|CVE-2020-0001|          escalation|\n",
      "|CVE-2020-0001|           privilege|\n",
      "|CVE-2020-0001|          additional|\n",
      "|CVE-2020-0001|           execution|\n",
      "|CVE-2020-0001|          privileges|\n",
      "|CVE-2020-0001|              needed|\n",
      "|CVE-2020-0001|                user|\n",
      "|CVE-2020-0001|         interaction|\n",
      "|CVE-2020-0001|              needed|\n",
      "|CVE-2020-0001|        exploitation|\n",
      "|CVE-2020-0001|             product|\n",
      "|CVE-2020-0001|             android|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def string_cleaner(input_str):\n",
    "    # 1. Replace all \".\" or ':' followed by whitespace with an empty string.\n",
    "    # a. Remove ending periods.\n",
    "    # 2. Remove trademark, rights.\n",
    "    # 3. Grab cotent in parentheses only.\n",
    "    # 4. Remove some punctuation.\n",
    "    cleaned_text = re.sub(r\"[.:,]+\\s+\", \" \", input_str)\n",
    "    # Remove trailing periods\n",
    "    cleaned_text = re.sub(r\"\\.$\", \"\", cleaned_text)\n",
    "    # Remove apostrophes, (TM), (R), parentheses, and double quotes, newlines\n",
    "    cleaned_text = re.sub(r\"\\'|\\(TM\\)|\\(R\\)|\\(|\\)|\\\"|[\\n]+\", \"\", cleaned_text)\n",
    "    # Convert to lowercase and strip leading/trailing spaces\n",
    "    cleaned_text = cleaned_text.lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "clean_tokens = descr_df.withColumn(\"token\", split(string_cleaner(col(\"description_single\")), \" \" ))\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"token\", outputCol=\"cleanToken\")\n",
    "clean_tokens = stop_words_remover.transform(clean_tokens)\n",
    "clean_tokens = clean_tokens.select(\"id\", explode(\"cleantoken\").alias(\"token\"))\n",
    "clean_tokens = clean_tokens.filter(col(\"token\").isNotNull())\n",
    "# Remove words that are only one character\n",
    "# https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "clean_tokens = clean_tokens.rdd.filter(lambda x: x['token'] != \"\")\n",
    "clean_tokens = clean_tokens.filter(lambda x: len(x['token']) > 2).toDF()\n",
    "\n",
    "id_tokens = clean_tokens\n",
    "id_tokens.cache()\n",
    "\n",
    "id_tokens.show()\n",
    "# # Show the resulting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a469f-5dca-4ba7-9ed7-c99179b1138d",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "In previous iterations of the application it became apparent that this dataset has its own set of stop words that causes TF-IDF Vectors (show in latter section), \n",
    "to have matches that were not as good as they could because of similar low importance words. \n",
    "\n",
    "Given that this is a dataset about CVEs words like 'attack', 'exploit', 'vulnerab*', don't convey as much meaning.  \n",
    "\n",
    "For example examine the top words after removing common stop words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6855082b-fce6-49e1-8297-c4c9c2d068b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|        token|  df|\n",
      "+-------------+----+\n",
      "|vulnerability|8523|\n",
      "|     attacker|6020|\n",
      "|       allows|5608|\n",
      "|          via|4681|\n",
      "|       remote|3847|\n",
      "|     versions|3713|\n",
      "|         user|3690|\n",
      "|         code|3632|\n",
      "|        issue|3367|\n",
      "|    arbitrary|3346|\n",
      "|       access|3181|\n",
      "|     affected|2809|\n",
      "|      crafted|2628|\n",
      "|        allow|2548|\n",
      "|    attackers|2485|\n",
      "|    execution|2450|\n",
      "|          use|2434|\n",
      "|          may|2407|\n",
      "|         file|2332|\n",
      "|      service|2264|\n",
      "|       exists|2222|\n",
      "|       system|2182|\n",
      "|  information|2104|\n",
      "|      exploit|2091|\n",
      "|         data|2035|\n",
      "|      execute|2023|\n",
      "|   discovered|1897|\n",
      "|      version|1862|\n",
      "|        prior|1812|\n",
      "|          xss|1796|\n",
      "|          due|1783|\n",
      "|   successful|1778|\n",
      "|       server|1755|\n",
      "|        cause|1736|\n",
      "|       number|1718|\n",
      "|        local|1684|\n",
      "|       memory|1663|\n",
      "|         lead|1622|\n",
      "|       reason|1614|\n",
      "|     rejected|1612|\n",
      "|authenticated|1611|\n",
      "|   privileges|1591|\n",
      "|    malicious|1582|\n",
      "|       denial|1575|\n",
      "|        notes|1567|\n",
      "|    candidate|1560|\n",
      "|   consultids|1548|\n",
      "|         none|1507|\n",
      "|          cna|1506|\n",
      "|         2020|1503|\n",
      "+-------------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\")).orderBy(desc(\"df\")).show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca5c75-1450-4b1e-ba8b-0978f08c3368",
   "metadata": {},
   "source": [
    "Lets resolve some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4575e416-304e-4130-b440-7c6a37d8801e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`df`, `token`].;\n'Aggregate [token#10810], [token#10810, 'count(distinct 'id) AS df#12934]\n+- Filter NOT token#10810 IN (attack,attacks,attacker,attackers,vulnerability,due,may,1,exploit,affects,affected,exists,version,versions,id)\n   +- Filter (df#10949L > cast(1 as bigint))\n      +- Sort [token#10810 ASC NULLS FIRST], true\n         +- Aggregate [token#10810], [token#10810, count(distinct id#10809) AS df#10949L]\n            +- Filter NOT token#10810 IN (attack,attacks,attacker,attackers,vulnerability,due,may,1,exploit,affects,affected,exists,version,versions,id)\n               +- LogicalRDD [id#10809, token#10810], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m clean_tokens \u001b[38;5;241m=\u001b[39m clean_tokens\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;241m~\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(\u001b[38;5;241m*\u001b[39mstopwordslist))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Sort the entire dataset and not partionwise.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m clean_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mclean_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountDistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Only include the tokens which occur in more than one document. \u001b[39;00m\n\u001b[1;32m      6\u001b[0m clean_tokens \u001b[38;5;241m=\u001b[39m clean_tokens\u001b[38;5;241m.\u001b[39mfilter(clean_tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `id` cannot be resolved. Did you mean one of the following? [`df`, `token`].;\n'Aggregate [token#10810], [token#10810, 'count(distinct 'id) AS df#12934]\n+- Filter NOT token#10810 IN (attack,attacks,attacker,attackers,vulnerability,due,may,1,exploit,affects,affected,exists,version,versions,id)\n   +- Filter (df#10949L > cast(1 as bigint))\n      +- Sort [token#10810 ASC NULLS FIRST], true\n         +- Aggregate [token#10810], [token#10810, count(distinct id#10809) AS df#10949L]\n            +- Filter NOT token#10810 IN (attack,attacks,attacker,attackers,vulnerability,due,may,1,exploit,affects,affected,exists,version,versions,id)\n               +- LogicalRDD [id#10809, token#10810], false\n"
     ]
    }
   ],
   "source": [
    "stopwordslist=['attack', 'attacks', 'attacker', 'attackers',  'vulnerability', 'due', 'may', '1','exploit', 'affects', 'affected', 'exists', 'version', 'versions', 'id' ]\n",
    "clean_tokens = clean_tokens.filter(~col(\"token\").isin(*stopwordslist))\n",
    "# Sort the entire dataset and not partionwise.\n",
    "clean_tokens = clean_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\")).orderBy(\"token\")\n",
    "# Only include the tokens which occur in more than one document. \n",
    "clean_tokens = clean_tokens.filter(clean_tokens['df'] > 1)\n",
    "\n",
    "# Cache the results, this will be used frequently.\n",
    "clean_tokens.cache()\n",
    "\n",
    "clean_tokens.show(n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "024cc724-5ce4-4134-aa42-6552b4c79cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT token)|\n",
      "+---------------------+\n",
      "|                15330|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.select(countDistinct(\"token\")).show()\n",
    "clean_tokens_count = clean_tokens.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6c08c89-0185-4eb1-a69f-b2adc73fe235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|               token|  df|           id|\n",
      "+--------------------+----+-------------+\n",
      "|activitymanagerse...|   3|CVE-2020-0001|\n",
      "|            isolated|  11|CVE-2020-0001|\n",
      "|                apps|  46|CVE-2020-0001|\n",
      "|             handled| 100|CVE-2020-0001|\n",
      "|           correctly|  74|CVE-2020-0001|\n",
      "|                lead|1622|CVE-2020-0001|\n",
      "|               local|1684|CVE-2020-0001|\n",
      "|          escalation| 746|CVE-2020-0001|\n",
      "|           privilege|1450|CVE-2020-0001|\n",
      "|          additional| 744|CVE-2020-0001|\n",
      "|           execution|2450|CVE-2020-0001|\n",
      "|          privileges|1591|CVE-2020-0001|\n",
      "|              needed| 519|CVE-2020-0001|\n",
      "|                user|3690|CVE-2020-0001|\n",
      "|         interaction| 990|CVE-2020-0001|\n",
      "|              needed| 519|CVE-2020-0001|\n",
      "|        exploitation| 477|CVE-2020-0001|\n",
      "|             product|1018|CVE-2020-0001|\n",
      "|             android| 265|CVE-2020-0001|\n",
      "|         android-8.0|  74|CVE-2020-0001|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the id's for which our tokens associated to it.\n",
    "id_tokens = clean_tokens.join(id_tokens,['token'],how='inner') \n",
    "id_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a2f8695-9403-4b47-a7ab-0d39a08edafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tokens, doc_count):\n",
    "    allTokensForId = tokens.groupBy(\"id\").agg(count(\"id\").alias(\"allTokensForId\"))\n",
    "\n",
    "    tfds = tokens.groupBy(\"id\", \"token\").agg(count(\"id\").alias(\"rawtf\"))\n",
    "    dfds = tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\"))\n",
    "\n",
    "    # Join the two DataFrames on 'id'\n",
    "    merged_df = tfds.join(allTokensForId, on=\"id\")\n",
    "\n",
    "    # Calculate the ratio of rawtf to allTokensForId\n",
    "    tfds = merged_df.withColumn(\"tf\", col(\"rawtf\") / col(\"allTokensForId\"))\n",
    "\n",
    "    merged_df.show()\n",
    "\n",
    "    # Define the UDF for idf calculation\n",
    "    spark.udf.register(\"calcidfudf\", lambda df: calcidf(doc_count, df), DoubleType())\n",
    "\n",
    "    # Calculate idf and add it as a new column \"idf\"\n",
    "    tokens_idf = dfds.withColumn(\"idf\", expr(\"calcidfudf(df)\"))\n",
    "\n",
    "    # Show the resulting dataframe\n",
    "    tfidfds = tokens_idf.join(tfds, \"token\", \"left\") \\\n",
    "        .withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "\n",
    "    return tfidfds\n",
    "\n",
    "def calcidf(doc_count, df):\n",
    "    # Calculate the tf-idf using natural log\n",
    "    return math.log((doc_count + 1.0) / (df + 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "396ff4a5-c2c5-4cee-99bb-e609153b7e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               token| df|\n",
      "+--------------------+---+\n",
      "|  !j@l#y$z%x6x7q8c9z|  2|\n",
      "|                 #gp|  2|\n",
      "|               $_get|  2|\n",
      "|              $_post|  3|\n",
      "|                 %0a|  2|\n",
      "|              %path%|  3|\n",
      "|%programfiles%\\1e...|  2|\n",
      "|               &amp;|  3|\n",
      "|        &quot;public|  2|\n",
      "|          &quot;safe|  2|\n",
      "|             **not**|  2|\n",
      "|        **resolved**|  2|\n",
      "|           **version|  5|\n",
      "|       *.example.com|  2|\n",
      "|               *ctxt|  2|\n",
      "|               *note| 19|\n",
      "|                 *pb|  2|\n",
      "|                 -->|  4|\n",
      "|            --config|  2|\n",
      "|--enable-experime...|  3|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-----------------+-----+--------------+\n",
      "|           id|            token|rawtf|allTokensForId|\n",
      "+-------------+-----------------+-----+--------------+\n",
      "|CVE-2020-0002|             code|    1|            24|\n",
      "|CVE-2020-0003|             user|    1|            23|\n",
      "|CVE-2020-0006|           needed|    2|            29|\n",
      "|CVE-2020-0022|             code|    1|            26|\n",
      "|CVE-2020-0022|        android-9|    1|            26|\n",
      "|CVE-2020-0029|             lead|    1|            22|\n",
      "|CVE-2020-0032|           needed|    2|            23|\n",
      "|CVE-2020-0033|            write|    1|            23|\n",
      "|CVE-2020-0038|        rw_i93.cc|    1|            26|\n",
      "|CVE-2020-0082|            local|    1|            20|\n",
      "|CVE-2020-0082|      interaction|    1|            20|\n",
      "|CVE-2020-0097|      interaction|    1|            24|\n",
      "|CVE-2020-0118|      interaction|    1|            20|\n",
      "|CVE-2020-0138|          missing|    1|            25|\n",
      "|CVE-2020-0165|            local|    1|            26|\n",
      "|CVE-2020-0167|           bounds|    1|            20|\n",
      "|CVE-2020-0173|       eas_mdls.c|    1|            21|\n",
      "|CVE-2020-0195|             user|    1|            21|\n",
      "|CVE-2020-0196|android-10android|    1|            22|\n",
      "|CVE-2020-0199|           needed|    2|            18|\n",
      "+-------------+-----------------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/22 16:26:55 WARN SimpleFunctionRegistry: The function calcidfudf replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|   token| df|              idf|            id|rawtf|allTokensForId|                  tf|             tf_idf|\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4342|    1|            15| 0.06666666666666667|0.49073229226029474|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4341|    1|            21|0.047619047619047616|0.35052306590021054|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-6364|    1|            36|0.027777777777777776|0.20447178844178948|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4322|    1|            27|0.037037037037037035| 0.2726290512557193|\n",
      "|    10.7| 12|7.360984383904421|CVE-2020-28846|    1|            13| 0.07692307692307693| 0.5662295679926478|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4459|    1|            23|0.043478260869565216|0.32004279930019225|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-6369|    1|            25|                0.04|0.29443937535617687|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4323|    1|            28| 0.03571428571428571| 0.2628922994251579|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-8113|    1|             7| 0.14285714285714285| 1.0515691977006316|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-5358|    1|            28| 0.03571428571428571| 0.2628922994251579|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4327|    1|            21|0.047619047619047616|0.35052306590021054|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4413|    1|            27|0.037037037037037035| 0.2726290512557193|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0595|    1|            22|0.045454545454545456|0.32808561546653403|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0540|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0538|    1|            20|                0.05|0.36089417701318743|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-8674|    1|            21|0.047619047619047616| 0.3437087400125594|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0531|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0596|    1|            23|0.043478260869565216| 0.3138210234897282|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0594|    1|            22|0.045454545454545456|0.32808561546653403|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0535|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.show()\n",
    "tfidf_df = tfidf(id_tokens, doc_count)\n",
    "\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1a1bb6a7-5725-4a2d-a4e9-0d3f144e1cf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Assuming you've already defined grouped_tfidf and tfidf_df DataFrames\u001b[39;00m\n\u001b[1;32m     20\u001b[0m grouped_tfidf \u001b[38;5;241m=\u001b[39m tfidf_df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(collect_list(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 21\u001b[0m tfidf_vector_with_sparse \u001b[38;5;241m=\u001b[39m grouped_tfidf\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_vector\u001b[39m\u001b[38;5;124m'\u001b[39m, sparse_vector_udf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_list\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m)\u001b[49m, lit(clean_tokens_count)))\n\u001b[1;32m     23\u001b[0m grouped_tfidf\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#docVect = create_sparse_vector(\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/functions.py:193\u001b[0m, in \u001b[0;36mlit\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m\"\u001b[39m, col)\u001b[38;5;241m.\u001b[39mastype(dt)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;28mstr\u001b[39m(col))\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/functions.py:97\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1314\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1314\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   1280\u001b[0m     temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1282\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1283\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1283\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m   1280\u001b[0m     temp_args \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1282\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1283\u001b[0m     [\u001b[43mget_command_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:298\u001b[0m, in \u001b[0;36mget_command_part\u001b[0;34m(parameter, python_proxy_pool)\u001b[0m\n\u001b[1;32m    296\u001b[0m         command_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m interface\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     command_part \u001b[38;5;241m=\u001b[39m REFERENCE_TYPE \u001b[38;5;241m+\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_id\u001b[49m()\n\u001b[1;32m    300\u001b[0m command_part \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m command_part\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3127\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \n\u001b[1;32m   3096\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3124\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Create a sparse vector for each of the documents in the dataset filled with 0's,\n",
    "    replacing the 0 with values that exist for that token on the id.\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "def create_sparse_vector(document_tf_idf, clean_tokens, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens\n",
    "    for i, token, idf in enumerate(clean_tokens):\n",
    "        for j, doc_token in enumerate(document_tf_idf):\n",
    "            if token == doc_token:\n",
    "                values[i] = idf\n",
    "\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "# Assuming you've already defined grouped_tfidf and tfidf_df DataFrames\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('token').alias('words'))\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('tfidf_list', lit(clean_tokens), lit(clean_tokens_count)))\n",
    "\n",
    "grouped_tfidf.show()\n",
    "\n",
    "#docVect = create_sparse_vector("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402679d-b95e-415f-93b5-d72d01fc264b",
   "metadata": {},
   "source": [
    "# IDEAS\n",
    "Already calculated the TF-IDF scores for the documents. Now, lets create a TF-IDF vector from these scores.\n",
    "\n",
    "Assuming you have a DataFrame named tfidf_vector with columns id, token, tf_idf, and allTokensForId, you can proceed as follows:\n",
    "\n",
    "    Group TF-IDF Scores by Document ID:\n",
    "        Group the DataFrame by the id column and aggregate the tf_idf values into a list for each document.\n",
    "        This will give you a list of TF-IDF scores for each document.\n",
    "\n",
    "    Create a Sparse Vector Representation:\n",
    "        Use the SparseVector class from PySpark to create a sparse vector representation for each document.\n",
    "        The vector will have dimensions equal to the total number of unique tokens (terms) in your dataset.\n",
    "        For each document, set the value at the index corresponding to the token to its corresponding TF-IDF score.\n",
    "\n",
    "    Assemble the Sparse Vectors:\n",
    "        Assemble the sparse vectors into a single column using the VectorAssembler.\n",
    "        This will give you a new DataFrame with a column containing the TF-IDF vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e936f122-60fa-4c3f-acc3-ec34140ba8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1077:>                                                       (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+------------------+\n",
      "|           id|               words|         rawFeatures|        normFeatures|        cosine_sim|\n",
      "+-------------+--------------------+--------------------+--------------------+------------------+\n",
      "|CVE-2020-0003|[android, interac...|(20000,[348,392,1...|(20000,[348,392,1...|1.0000000000000002|\n",
      "|CVE-2020-0215|[android, interac...|(20000,[104,392,1...|(20000,[104,392,1...|0.6366550033321677|\n",
      "|CVE-2020-0009|[a-142938932, cor...|(20000,[392,1662,...|(20000,[392,1662,...|0.6304883249912806|\n",
      "|CVE-2020-0001|[android-8.1, nee...|(20000,[270,392,2...|(20000,[270,392,2...| 0.595879571531124|\n",
      "|CVE-2020-0202|[android, develop...|(20000,[392,1662,...|(20000,[392,1662,...|0.5910828046793257|\n",
      "+-------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This is just to compare how my vectorization method compares to ML libs \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, Normalizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('token').alias('words'))\n",
    "ht = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "featurizedData = ht.transform(grouped_tfidf)\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"rawFeatures\", outputCol=\"normFeatures\")\n",
    "normData = normalizer.transform(featurizedData)\n",
    "\n",
    "first_doc_features = normData.filter(col('id') == 'CVE-2020-0003').select('normFeatures').first()[0]\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Broadcast the first document's features\n",
    "first_doc_features_bc = sc.broadcast(first_doc_features)\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "def cosine_similarity(v):\n",
    "    return float(first_doc_features_bc.value.dot(v) / (first_doc_features_bc.value.norm(2) * v.norm(2)))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Compute cosine similarity\n",
    "result = normData.withColumn(\"cosine_sim\", cosine_similarity_udf(col('normFeatures')))\n",
    "\n",
    "# Get the top similar documents\n",
    "top_similar_docs = result.sort(col(\"cosine_sim\").desc()).limit(5)\n",
    "\n",
    "top_similar_docs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37dd189f-0b27-40ab-ae58-f8629e318579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|   token| df|              idf|            id|rawtf|allTokensForId|                  tf|             tf_idf|\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4342|    1|            15| 0.06666666666666667|0.49073229226029474|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4341|    1|            21|0.047619047619047616|0.35052306590021054|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-6364|    1|            36|0.027777777777777776|0.20447178844178948|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4322|    1|            27|0.037037037037037035| 0.2726290512557193|\n",
      "|    10.7| 12|7.360984383904421|CVE-2020-28846|    1|            13| 0.07692307692307693| 0.5662295679926478|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4459|    1|            23|0.043478260869565216|0.32004279930019225|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-6369|    1|            25|                0.04|0.29443937535617687|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4323|    1|            28| 0.03571428571428571| 0.2628922994251579|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-8113|    1|             7| 0.14285714285714285| 1.0515691977006316|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-5358|    1|            28| 0.03571428571428571| 0.2628922994251579|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4327|    1|            21|0.047619047619047616|0.35052306590021054|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4413|    1|            27|0.037037037037037035| 0.2726290512557193|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0595|    1|            22|0.045454545454545456|0.32808561546653403|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0540|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0538|    1|            20|                0.05|0.36089417701318743|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-8674|    1|            21|0.047619047619047616| 0.3437087400125594|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0531|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0596|    1|            23|0.043478260869565216| 0.3138210234897282|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0594|    1|            22|0.045454545454545456|0.32808561546653403|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0535|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'collect_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row\n\u001b[1;32m      3\u001b[0m tfidf_df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 5\u001b[0m grouped_tfidf \u001b[38;5;241m=\u001b[39m tfidf_df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\u001b[43mcollect_list\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m     collect_list(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_idf\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_list\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msort_words\u001b[39m(record):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mid\u001b[39m, words, tf_idf \u001b[38;5;241m=\u001b[39m record\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collect_list' is not defined"
     ]
    }
   ],
   "source": [
    "# I don't think I need this anymore\n",
    "from pyspark.sql import Row\n",
    "tfidf_df.show()\n",
    "\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('token').alias('document'),\n",
    "    collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "def sort_words(record):\n",
    "    id, words, tf_idf = record\n",
    "    sorted_words = sorted(words)\n",
    "    sorted_tf_idf = [x for _, x in sorted(zip(words, tf_idf))]\n",
    "    return Row(id=id, words_list=sorted_words, tfidf_list=sorted_tf_idf)\n",
    "\n",
    "rdd = grouped_tfidf.rdd.map(sort_words)\n",
    "\n",
    "grouped_tfidf = rdd.toDF()\n",
    "\n",
    "grouped_tfidf.show(truncate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6c381-5054-4c8b-8cdf-acdce01f1f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97b7131c-23d2-4ba2-a8fb-f579cb838370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 955:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------------------------------------------------+\n",
      "|           id|                                                                                      tfidf_features|\n",
      "+-------------+----------------------------------------------------------------------------------------------------+\n",
      "|CVE-2020-0003|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],[0.3776032550073269,0.1316182263928...|\n",
      "|CVE-2020-0012|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.34972427902766523,0.12158035118864902,...|\n",
      "|CVE-2020-0028|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.14050094351952408,0.144153295573141...|\n",
      "|CVE-2020-0031|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.12828347017000025,0.131618226392868...|\n",
      "|CVE-2020-0042|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.2067827348849647,0.14050094351952408,0...|\n",
      "|CVE-2020-0050|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],[0.13265150579155024,0.137600873047...|\n",
      "|CVE-2020-0064|(15330,[0,1,2,3,4,5,6],[0.6203482046548942,0.9321051942434003,0.6380497467459713,0.43507852273999...|\n",
      "|CVE-2020-0067|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.5921505589887626,0.13411453699590936,0...|\n",
      "|CVE-2020-0068|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.5664048825109903,0.1282834701700002...|\n",
      "|CVE-2020-0076|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.16701682433016382,0.116...|\n",
      "|CVE-2020-0084|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.12613413362649875,0.104017...|\n",
      "|CVE-2020-0091|(15330,[0,1,2,3,4,5,6],[0.6203482046548942,0.9321051942434003,0.5660506282496445,1.01324577104424...|\n",
      "|CVE-2020-0109|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],[0.13161822639286827,0.108539691...|\n",
      "|CVE-2020-0116|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],[0.14050094351952408,0.14415329557314144,0.1...|\n",
      "|CVE-2020-0124|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],[0.23447458892680048,0.3672104929790485,0.12...|\n",
      "|CVE-2020-0125|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],[0.1475259906955003,0.15136096035179852,0.12...|\n",
      "|CVE-2020-0139|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.12293832557958358,0.126134...|\n",
      "|CVE-2020-0140|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],[0.28100188703904816,0.14415329557314144,0.2...|\n",
      "|CVE-2020-0142|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.2682290739918187,0.13760087304708957,0...|\n",
      "|CVE-2020-0146|(15330,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.11802079255640024,0.121088...|\n",
      "+-------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "# Create Sparse Vectors\n",
    "def create_sparse_vector(tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens  # Initialize all values to 0.0\n",
    "    for i, tfidf_score in enumerate(tfidf_list):\n",
    "        values[i] = tfidf_score\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('tfidf_list', lit(clean_tokens_count)))\n",
    "\n",
    "# Assemble the Sparse Vectors\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_tfidf_vector.select('id', 'tfidf_features').show(truncate=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaf3fd-3797-4f44-9043-bb762e1ac213",
   "metadata": {},
   "source": [
    "Now that you have the TF-IDF vectors for the documents, here are some useful things to do with the results:\n",
    "\n",
    "    Document Similarity:\n",
    "        Calculate the similarity between documents using cosine similarity or other distance metrics. The closer the vectors, the more similar the documents.\n",
    "        For example, you can find similar documents to a given query document by comparing their TF-IDF vectors.\n",
    "\n",
    "    Topic Modeling:\n",
    "        Apply topic modeling techniques (such as Latent Dirichlet Allocation or Non-Negative Matrix Factorization) to discover underlying topics in your corpus.\n",
    "        Use the TF-IDF vectors as input for these models.\n",
    "\n",
    "    Classification and Clustering:\n",
    "        Train machine learning models (e.g., SVM, Random Forest, or k-means) using the TF-IDF vectors as features.\n",
    "        Classify documents into predefined categories or cluster similar documents together.\n",
    "\n",
    "    Keyword Extraction:\n",
    "        Identify important keywords or phrases within each document based on their TF-IDF scores.\n",
    "        Higher TF-IDF scores indicate more significant terms.\n",
    "\n",
    "    Search and Retrieval:\n",
    "        Use the TF-IDF vectors to build an efficient search index for your documents.\n",
    "        Given a query, retrieve relevant documents based on their similarity to the query.\n",
    "\n",
    "    Visualizations:\n",
    "        Visualize the TF-IDF vectors in lower dimensions using techniques like t-SNE or PCA.\n",
    "        Explore the distribution of documents in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e097ca7a-c628-4d7f-a655-76d14c28d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize the vectors\n",
    "normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"normFeatures\")\n",
    "data = normalizer.transform(final_tfidf_vector)\n",
    "\n",
    "# Get the first document's features\n",
    "first_doc_features = data.first().tfidf_features\n",
    "def cosine_similarity(v):\n",
    "    dot_product = float(first_doc_features.dot(v))\n",
    "    norm_product = float(first_doc_features.norm(2)) * float(v.norm(2))\n",
    "    return float(dot_product) / float(norm_product)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "# Compute the cosine similarity and add it as a new column\n",
    "data = data.withColumn(\"cosine_sim\", cosine_similarity_udf(col(\"normFeatures\")))\n",
    "\n",
    "# Show the top 5 documents\n",
    "#data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f64e0d-afdb-464d-a680-97665b63017b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 13:05:46 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "[Stage 351:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|            id|cosine_sim|\n",
      "+--------------+----------+\n",
      "| CVE-2020-0003|       1.0|\n",
      "|CVE-2020-18475| 0.9601347|\n",
      "|CVE-2020-15007|0.95723075|\n",
      "|CVE-2020-36034| 0.9530993|\n",
      "| CVE-2020-0218| 0.9470317|\n",
      "+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"cosine_sim2\", cos_sim_udf(col(\"normFeatures\")))\n",
    "data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80254d-b90f-4df1-95da-41f423b18bdb",
   "metadata": {},
   "source": [
    "# The results:\n",
    "## Results from first iteration - No stop words were removed\n",
    "### CVE-2020-0003 (Search Document)\n",
    "In onCreate of InstallStart.java, there is a possible package validation bypass due to a time-of-check time-of-use vulnerability. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is needed for exploitation. Product: Android Versions: Android-8.0 Android ID: A-140195904\n",
    "\n",
    "### CVE-2020-25162\n",
    "A XPath injection vulnerability in the B. Braun Melsungen AG SpaceCom Version L81/U61 and earlier, and the Data module compactplus Versions A10 and A11 allows unauthenticated remote attackers to access sensitive information and escalate privileges\n",
    "\n",
    "### CVE-2020-24721\n",
    "An issue was discovered in the GAEN (aka Google/Apple Exposure Notifications) protocol through 2020-09-29, as used in COVID-19 applications on Android and iOS. It allows a user to be put in a position where he or she can be coerced into proving or disproving an exposure notification, because of the persistent state of a private framework.\n",
    "\n",
    "There could be a few reasons why the documents getting aren't as similar as I'd like. Here are some things to consider:\n",
    "\n",
    "1. **Quality of your data**: The quality and relevance of the documents in your dataset can greatly affect the results of cosine similarity. If the documents in your dataset are not very similar to your target document to begin with, the top results might not seem very similar.\n",
    "\n",
    "2. **Preprocessing**: How you preprocess your data can also affect the results. This includes things like removing stop words, stemming or lemmatization, and how you handle punctuation and capitalization. You might need to adjust your preprocessing steps to better suit your data.\n",
    "\n",
    "3. **Vectorization**: The method you use to convert your text data into numerical vectors can also have a big impact. You're currently using TF-IDF, which is a good choice for many applications, but there might be other methods that work better for your specific dataset. You could experiment with other methods like Word2Vec, Doc2Vec, or BERT embeddings.\n",
    "\n",
    "4. **Dimensionality**: High-dimensional data can be problematic for cosine similarity due to the curse of dimensionality. You might want to try reducing the dimensionality of your data with techniques like PCA or t-SNE.\n",
    "\n",
    "5. **Thresholding**: You might want to consider applying a threshold to your cosine similarity scores. This means that you only consider documents as \"similar\" if their cosine similarity score is above a certain threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102ee23-5906-4781-95a5-f514033ecd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f82bcc-458e-4599-9b31-bb781f28bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a829a-d5c9-4d6a-be16-8903b6521971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc47cd9-209a-4370-9672-02419a964db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
