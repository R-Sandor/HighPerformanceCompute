{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25839c2-9d92-4789-9dd7-353952d23077",
   "metadata": {},
   "source": [
    "# Volt\n",
    "# Summary \n",
    "In this project the National Vulnerability Database's (NVD) of Common exposures and vulnerabilites (CVES) is levarged\n",
    "to create a tool that is used as a part of the data pipeline to determine how similar CVE's are to one another. \n",
    "\n",
    "The first goal to such a pipeline is to clean the description data, tokenize the data. The second goal is tokenize and read the data that will be used as a training label. \n",
    "## Project Objective: \n",
    "### Create a Pipeline that Can Support Document Similarity and Search\n",
    "\n",
    "\n",
    "# Requirements\n",
    "Required packages:\n",
    "- pyspark\n",
    "- numpy\n",
    "\n",
    "TF-IDF use cases:\n",
    "- LDA\n",
    "- Similarity with\n",
    "1. **Feature Selection**:\n",
    "   - Examine the top TF-IDF terms to identify important features. You can select a subset of these features for further analysis or modeling.\n",
    "   - Consider removing low-TF-IDF terms (common words) that might not contribute significantly to your task.\n",
    "\n",
    "2. **Clustering and Topic Modeling**:\n",
    "   - Apply clustering algorithms (e.g., K-means, DBSCAN) to group similar documents based on their TF-IDF vectors.\n",
    "   - Explore topic modeling techniques (e.g., Latent Dirichlet Allocation, Non-Negative Matrix Factorization) to discover latent topics within your corpus.\n",
    "\n",
    "3. **Document Similarity**:\n",
    "   - Calculate cosine similarity between TF-IDF vectors of different documents. This helps identify similar documents.\n",
    "   - Use similarity scores to recommend related articles, products, or content.\n",
    "\n",
    "4. **Classification and Sentiment Analysis**:\n",
    "   - Train classifiers (e.g., SVM, Random Forest) using TF-IDF features as input. This is useful for tasks like sentiment analysis, spam detection, or document categorization.\n",
    "   - Convert text data into TF-IDF vectors and use them as features for machine learning models.\n",
    "\n",
    "5. **Search and Information Retrieval**:\n",
    "   - Build an inverted index using TF-IDF vectors to create an efficient search engine.\n",
    "   - Retrieve relevant documents based on user queries by ranking them using their TF-IDF scores.\n",
    "\n",
    "6. **Visualizations**:\n",
    "   - Visualize TF-IDF scores using word clouds, scatter plots, or bar charts to gain insights into term importance.\n",
    "   - Plot the distribution of TF-IDF values across the entire dataset.\n",
    "\n",
    "7. **Optimize Hyperparameters**:\n",
    "   - Experiment with different parameters (e.g., n-grams, stop words, max features) in your TF-IDF vectorization process.\n",
    "   - Use cross-validation to find optimal settings.\n",
    "\n",
    "TODO: Find sources to back up these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec292ec-4971-401b-bb8a-df0d79a467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python system utilities\n",
    "import os, math, re\n",
    "# Pyspark for Spark ðŸŒŸ\n",
    "import pyspark\n",
    "# Used to download the dataset the first time\n",
    "import urllib.request\n",
    "import zipfile\n",
    "# Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "# Common functions.\n",
    "from pyspark.sql.functions import col, count, collect_list, countDistinct, concat_ws, desc, explode, expr, lit, udf, split, sum\n",
    "# Spark data types used throughout the application.\n",
    "from pyspark.sql.types import DoubleType, FloatType, StringType, ArrayType\n",
    "# Used to remove common stopwords. We do additional dataset specific stopword analysis latter.\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# These are spark math libraries that allow the developer to make memory effienct vectors\n",
    "from pyspark.ml.linalg import VectorUDT, SparseVector\n",
    "# This takes a sparse vector and gets the features from the vector.\n",
    "from pyspark.ml.feature import VectorAssembler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d229f2-1456-48f6-933b-7aa286239fb8",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Handle downloading the data if it doesn't already exist for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abf5b21-76a6-45a8-862b-ab0e82b43e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset if it doesn't already exist. \n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True) \n",
    "fileUrls = [\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2003.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2004.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2005.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2006.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2007.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2008.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2009.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2010.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2011.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-recent.json.zip'\n",
    "    ]\n",
    "\n",
    "# Iterate through each URL\n",
    "for url in fileUrls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    outputfile = os.path.join(data_dir, filename)\n",
    "    checkfile = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(checkfile):\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, outputfile)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "        # Extract the file\n",
    "        with zipfile.ZipFile(outputfile, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "        # Delete the original zip file\n",
    "        os.remove(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0844e87-acd6-484f-b351-063213f0be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 13:30:57 WARN Utils: Your hostname, sandbox resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface eth0)\n",
      "24/03/23 13:30:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/23 13:30:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CVE_Items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- configurations: struct (nullable = true)\n",
      " |    |    |    |-- CVE_data_version: string (nullable = true)\n",
      " |    |    |    |-- nodes: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- cpe_match: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- cpe23Uri: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- cpe_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionEndExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionEndIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionStartExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionStartIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- vulnerable: boolean (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- operator: string (nullable = true)\n",
      " |    |    |    |    |    |-- cpe_match: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- cpe23Uri: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- cpe_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- versionEndExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionEndIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionStartExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionStartIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- vulnerable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- operator: string (nullable = true)\n",
      " |    |    |-- cve: struct (nullable = true)\n",
      " |    |    |    |-- CVE_data_meta: struct (nullable = true)\n",
      " |    |    |    |    |-- ASSIGNER: string (nullable = true)\n",
      " |    |    |    |    |-- ID: string (nullable = true)\n",
      " |    |    |    |-- data_format: string (nullable = true)\n",
      " |    |    |    |-- data_type: string (nullable = true)\n",
      " |    |    |    |-- data_version: string (nullable = true)\n",
      " |    |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |    |-- description_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- problemtype: struct (nullable = true)\n",
      " |    |    |    |    |-- problemtype_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- description: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- references: struct (nullable = true)\n",
      " |    |    |    |    |-- reference_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- refsource: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- impact: struct (nullable = true)\n",
      " |    |    |    |-- baseMetricV2: struct (nullable = true)\n",
      " |    |    |    |    |-- acInsufInfo: boolean (nullable = true)\n",
      " |    |    |    |    |-- cvssV2: struct (nullable = true)\n",
      " |    |    |    |    |    |-- accessComplexity: string (nullable = true)\n",
      " |    |    |    |    |    |-- accessVector: string (nullable = true)\n",
      " |    |    |    |    |    |-- authentication: string (nullable = true)\n",
      " |    |    |    |    |    |-- availabilityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- baseScore: double (nullable = true)\n",
      " |    |    |    |    |    |-- confidentialityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- integrityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- vectorString: string (nullable = true)\n",
      " |    |    |    |    |    |-- version: string (nullable = true)\n",
      " |    |    |    |    |-- exploitabilityScore: double (nullable = true)\n",
      " |    |    |    |    |-- impactScore: double (nullable = true)\n",
      " |    |    |    |    |-- obtainAllPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- obtainOtherPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- obtainUserPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- severity: string (nullable = true)\n",
      " |    |    |    |    |-- userInteractionRequired: boolean (nullable = true)\n",
      " |    |    |    |-- baseMetricV3: struct (nullable = true)\n",
      " |    |    |    |    |-- cvssV3: struct (nullable = true)\n",
      " |    |    |    |    |    |-- attackComplexity: string (nullable = true)\n",
      " |    |    |    |    |    |-- attackVector: string (nullable = true)\n",
      " |    |    |    |    |    |-- availabilityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- baseScore: double (nullable = true)\n",
      " |    |    |    |    |    |-- baseSeverity: string (nullable = true)\n",
      " |    |    |    |    |    |-- confidentialityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- integrityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- privilegesRequired: string (nullable = true)\n",
      " |    |    |    |    |    |-- scope: string (nullable = true)\n",
      " |    |    |    |    |    |-- userInteraction: string (nullable = true)\n",
      " |    |    |    |    |    |-- vectorString: string (nullable = true)\n",
      " |    |    |    |    |    |-- version: string (nullable = true)\n",
      " |    |    |    |    |-- exploitabilityScore: double (nullable = true)\n",
      " |    |    |    |    |-- impactScore: double (nullable = true)\n",
      " |    |    |-- lastModifiedDate: string (nullable = true)\n",
      " |    |    |-- publishedDate: string (nullable = true)\n",
      " |-- CVE_data_format: string (nullable = true)\n",
      " |-- CVE_data_numberOfCVEs: string (nullable = true)\n",
      " |-- CVE_data_timestamp: string (nullable = true)\n",
      " |-- CVE_data_type: string (nullable = true)\n",
      " |-- CVE_data_version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"voltcve\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.default.parallelism\", 8)\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "        .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"5g\") \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "# read the data\n",
    "cves = spark.read.option(\"multiline\", \"true\").json(\"data/nvdcve-1.1-2020.json\")\n",
    "\n",
    "# Manipulate the data to be more usable \n",
    "cves.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6172a-4a1e-46f4-8920-697f3793ff3b",
   "metadata": {},
   "source": [
    "# Complex JSON\n",
    "Look at the Schema above, the data is complex and Spark can read the data into a DataFrame. The DataFrame being equvielent to Spark SQL. \n",
    "\n",
    "From the complex schema the data needs to be turned into a more usable format. \n",
    "\n",
    "## Section Objective:\n",
    "- Get Ids\n",
    "- Description Data\n",
    "- Remove unnecessary nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32967fa-907f-4385-86a4-07a81cee8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = cves.select(explode(col(\"CVE_Items\")).alias(\"cves\"))\n",
    "\n",
    "descr_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"),\n",
    "          col(\"cves.cve.description.description_data.value\").alias(\"description\"));\n",
    "\n",
    "descr_df = descr_df.withColumn(\"description_single\", concat_ws(\" \", descr_df[\"description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e1de82-2fce-480c-b534-a40392dc6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 20453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_count = descr_df.selectExpr(\"count(distinct id)\").first()[0]\n",
    "print(\"Number of docs: {}\".format(doc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a2b2a-a08d-4ae5-ac04-68e27baadf8a",
   "metadata": {},
   "source": [
    "# CVE Product Info\n",
    "Below the goal is to get useful information about the CVEs. For example, the type of tool or software that had a vulnerability.\n",
    "- Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "- If it is not used in the application the data may be used in other applications the data later.\n",
    "\n",
    "\n",
    "Example of CPEs: \\\n",
    "cpe:2.3:a:ntp:ntp:4.2.8:p3:*:*:*:*:*:* \\\n",
    "cpe:2.3:o:microsoft:windows_7:-:sp2:*:*:*:*:*:* \\\n",
    "cpe:2.3:a:microsoft:internet_explorer:8.0.6001:beta:*:*:*:*:*:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc9d497-4eab-40fc-a888-62af78b67b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|           id|product|\n",
      "+-------------+-------+\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0003|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0009|android|\n",
      "|CVE-2020-0010|android|\n",
      "|CVE-2020-0011|android|\n",
      "|CVE-2020-0012|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0016|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "+-------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now 'exploded' contains individual rows for each 'CVE_Item'\n",
    "# Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "# cpe:<cpe_version>:<part>:<vendor>:<product>:<version>:<update>:<edition>:<language>:<sw_edition>:<target_sw>:<target_hw>:<other>\n",
    "cpe_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"), explode(col(\"cves.configurations.nodes.cpe_match\")[0]).alias(\"cpe\"))\n",
    "\n",
    "cpe_df = cpe_df.select(col(\"id\"), col(\"cpe\").alias(\"cpe\"))\n",
    "cpe_df = cpe_df.select(\"id\", split(cpe_df.cpe.cpe23uri,\":\",-1)[4].alias(\"product\"))\n",
    "\n",
    "# Sample of the data - schema/show\n",
    "cpe_df.printSchema()\n",
    "cpe_df.show(n=50, truncate=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5f30d-ca76-4915-b2a7-fe3fa3c6e0b0",
   "metadata": {},
   "source": [
    "## Uses from the cpe data:\n",
    "- CPE could be a training label.\n",
    "- CPE used in clustering.\n",
    "- Document filtering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a058d1-e741-479e-9e49-4fedc15f71f8",
   "metadata": {},
   "source": [
    "# Tokenization: \n",
    "## Applying the right tokenization methods:\n",
    "The dataset contains a lot of data the ordinary tokenization may not apply. For example, file names contain puncutation and can leave the token meaningless. \n",
    "An effort was made to preserve all meaninful punction that would describe software or hardware configurations, while removing stop words, and ordinary english punctation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850909ac-b461-45b6-8d55-327f2d71609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|           id|               token|\n",
      "+-------------+--------------------+\n",
      "|CVE-2020-0001|getprocessrecordl...|\n",
      "|CVE-2020-0001|activitymanagerse...|\n",
      "|CVE-2020-0001|            isolated|\n",
      "|CVE-2020-0001|                apps|\n",
      "|CVE-2020-0001|             handled|\n",
      "|CVE-2020-0001|           correctly|\n",
      "|CVE-2020-0001|                lead|\n",
      "|CVE-2020-0001|               local|\n",
      "|CVE-2020-0001|          escalation|\n",
      "|CVE-2020-0001|           privilege|\n",
      "|CVE-2020-0001|          additional|\n",
      "|CVE-2020-0001|           execution|\n",
      "|CVE-2020-0001|          privileges|\n",
      "|CVE-2020-0001|              needed|\n",
      "|CVE-2020-0001|                user|\n",
      "|CVE-2020-0001|         interaction|\n",
      "|CVE-2020-0001|              needed|\n",
      "|CVE-2020-0001|        exploitation|\n",
      "|CVE-2020-0001|             product|\n",
      "|CVE-2020-0001|             android|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def string_cleaner(input_str):\n",
    "    # 1. Replace all \".\" or ':' followed by whitespace with an empty string.\n",
    "    # a. Remove ending periods.\n",
    "    # 2. Remove trademark, rights.\n",
    "    # 3. Grab cotent in parentheses only.\n",
    "    # 4. Remove some punctuation.\n",
    "    cleaned_text = re.sub(r\"[.:,]+\\s+\", \" \", input_str)\n",
    "    # Remove trailing periods\n",
    "    cleaned_text = re.sub(r\"\\.$\", \"\", cleaned_text)\n",
    "    # Remove apostrophes, (TM), (R), parentheses, and double quotes, newlines\n",
    "    cleaned_text = re.sub(r\"\\'|\\(TM\\)|\\(R\\)|\\(|\\)|\\\"|[\\n]+\", \"\", cleaned_text)\n",
    "    # Convert to lowercase and strip leading/trailing spaces\n",
    "    cleaned_text = cleaned_text.lower().strip()\n",
    "    return cleaned_text\n",
    "\n",
    "clean_tokens = descr_df.withColumn(\"token\", split(string_cleaner(col(\"description_single\")), \" \" ))\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"token\", outputCol=\"cleanToken\")\n",
    "clean_tokens = stop_words_remover.transform(clean_tokens)\n",
    "clean_tokens = clean_tokens.select(\"id\", explode(\"cleantoken\").alias(\"token\"))\n",
    "clean_tokens = clean_tokens.filter(col(\"token\").isNotNull())\n",
    "# Remove words that are only one character\n",
    "# https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "clean_tokens = clean_tokens.rdd.filter(lambda x: x['token'] != \"\")\n",
    "clean_tokens = clean_tokens.filter(lambda x: len(x['token']) > 2).toDF()\n",
    "\n",
    "id_tokens = clean_tokens\n",
    "id_tokens = id_tokens.cache()\n",
    "\n",
    "id_tokens.show()\n",
    "# # Show the resulting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a469f-5dca-4ba7-9ed7-c99179b1138d",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "In previous iterations of the application it became apparent that this dataset has its own set of stop words that causes TF-IDF Vectors (show in latter section), \n",
    "to have matches that were not as good as they could because of similar low importance words. \n",
    "\n",
    "Given that this is a dataset about CVEs words like 'attack', 'exploit', 'vulnerab*', don't convey as much meaning.  \n",
    "\n",
    "For example examine the top words after removing common stop words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6855082b-fce6-49e1-8297-c4c9c2d068b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|        token|  df|\n",
      "+-------------+----+\n",
      "|vulnerability|8523|\n",
      "|     attacker|6020|\n",
      "|       allows|5608|\n",
      "|          via|4681|\n",
      "|       remote|3847|\n",
      "|     versions|3713|\n",
      "|         user|3690|\n",
      "|         code|3632|\n",
      "|        issue|3367|\n",
      "|    arbitrary|3346|\n",
      "|       access|3181|\n",
      "|     affected|2809|\n",
      "|      crafted|2628|\n",
      "|        allow|2548|\n",
      "|    attackers|2485|\n",
      "|    execution|2450|\n",
      "|          use|2434|\n",
      "|          may|2407|\n",
      "|         file|2332|\n",
      "|      service|2264|\n",
      "|       exists|2222|\n",
      "|       system|2182|\n",
      "|  information|2104|\n",
      "|      exploit|2091|\n",
      "|         data|2035|\n",
      "|      execute|2023|\n",
      "|   discovered|1897|\n",
      "|      version|1862|\n",
      "|        prior|1812|\n",
      "|          xss|1796|\n",
      "|          due|1783|\n",
      "|   successful|1778|\n",
      "|       server|1755|\n",
      "|        cause|1736|\n",
      "|       number|1718|\n",
      "|        local|1684|\n",
      "|       memory|1663|\n",
      "|         lead|1622|\n",
      "|       reason|1614|\n",
      "|     rejected|1612|\n",
      "|authenticated|1611|\n",
      "|   privileges|1591|\n",
      "|    malicious|1582|\n",
      "|       denial|1575|\n",
      "|        notes|1567|\n",
      "|    candidate|1560|\n",
      "|   consultids|1548|\n",
      "|         none|1507|\n",
      "|          cna|1506|\n",
      "|         2020|1503|\n",
      "+-------------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\")).orderBy(desc(\"df\")).show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca5c75-1450-4b1e-ba8b-0978f08c3368",
   "metadata": {},
   "source": [
    "Lets resolve some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4575e416-304e-4130-b440-7c6a37d8801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:======================================>               (144 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               token| df|\n",
      "+--------------------+---+\n",
      "|  !j@l#y$z%x6x7q8c9z|  2|\n",
      "|                 #gp|  2|\n",
      "|               $_get|  2|\n",
      "|              $_post|  3|\n",
      "|                 %0a|  2|\n",
      "|              %path%|  3|\n",
      "|%programfiles%\\1e...|  2|\n",
      "|               &amp;|  3|\n",
      "|        &quot;public|  2|\n",
      "|          &quot;safe|  2|\n",
      "|             **not**|  2|\n",
      "|        **resolved**|  2|\n",
      "|           **version|  5|\n",
      "|       *.example.com|  2|\n",
      "|               *ctxt|  2|\n",
      "|               *note| 19|\n",
      "|                 *pb|  2|\n",
      "|                 -->|  4|\n",
      "|            --config|  2|\n",
      "|--enable-experime...|  3|\n",
      "|            --output|  2|\n",
      "|              -dsafe|  2|\n",
      "|                 -s+|  5|\n",
      "|                 ...|  2|\n",
      "|                 ../| 20|\n",
      "|../../programs/dw...|  2|\n",
      "|                 ..\\|  3|\n",
      "|             .append|  2|\n",
      "|                .bat|  2|\n",
      "|                .bmp|  2|\n",
      "|                .bss|  2|\n",
      "|                 .cf|  3|\n",
      "|                .dll|  4|\n",
      "|               .docx|  4|\n",
      "|                .e70|  2|\n",
      "|                .exe|  9|\n",
      "|                .fla|  4|\n",
      "|           .htaccess|  6|\n",
      "|               .html|  5|\n",
      "|                .ico|  2|\n",
      "|                .lnk|  5|\n",
      "|                 .md|  2|\n",
      "|                .net| 20|\n",
      "|              .noexe|  2|\n",
      "|                .pdf|  5|\n",
      "|               .phar|  4|\n",
      "|                .php| 31|\n",
      "|               .php7|  2|\n",
      "|                .pht|  2|\n",
      "|              .phtml|  2|\n",
      "+--------------------+---+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stopwordslist=['attack', 'attacks', 'attacker', 'attackers',  'vulnerability', 'due', 'may', '1','exploit', 'affects', 'affected', 'exists', 'version', 'versions', 'id' ]\n",
    "clean_tokens = clean_tokens.filter(~col(\"token\").isin(*stopwordslist))\n",
    "# Sort the entire dataset and not partionwise.\n",
    "clean_tokens = clean_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\")).orderBy(\"token\")\n",
    "# Only include the tokens which occur in more than one document. \n",
    "clean_tokens = clean_tokens.filter(clean_tokens['df'] > 1)\n",
    "\n",
    "# Cache the results, this will be used frequently.\n",
    "clean_token = clean_tokens.cache()\n",
    "\n",
    "clean_tokens.show(n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024cc724-5ce4-4134-aa42-6552b4c79cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(DISTINCT token)|\n",
      "+---------------------+\n",
      "|                15330|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.select(countDistinct(\"token\")).show()\n",
    "clean_tokens_count = clean_tokens.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6c08c89-0185-4eb1-a69f-b2adc73fe235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|               token|  df|           id|\n",
      "+--------------------+----+-------------+\n",
      "|activitymanagerse...|   3|CVE-2020-0001|\n",
      "|            isolated|  11|CVE-2020-0001|\n",
      "|                apps|  46|CVE-2020-0001|\n",
      "|             handled| 100|CVE-2020-0001|\n",
      "|           correctly|  74|CVE-2020-0001|\n",
      "|                lead|1622|CVE-2020-0001|\n",
      "|               local|1684|CVE-2020-0001|\n",
      "|          escalation| 746|CVE-2020-0001|\n",
      "|           privilege|1450|CVE-2020-0001|\n",
      "|          additional| 744|CVE-2020-0001|\n",
      "|           execution|2450|CVE-2020-0001|\n",
      "|          privileges|1591|CVE-2020-0001|\n",
      "|              needed| 519|CVE-2020-0001|\n",
      "|                user|3690|CVE-2020-0001|\n",
      "|         interaction| 990|CVE-2020-0001|\n",
      "|              needed| 519|CVE-2020-0001|\n",
      "|        exploitation| 477|CVE-2020-0001|\n",
      "|             product|1018|CVE-2020-0001|\n",
      "|             android| 265|CVE-2020-0001|\n",
      "|         android-8.0|  74|CVE-2020-0001|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the id's for which our tokens associated to it.\n",
    "id_tokens = clean_tokens.join(id_tokens,['token'],how='inner') \n",
    "id_tokens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a2f8695-9403-4b47-a7ab-0d39a08edafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tokens, doc_count):\n",
    "    allTokensForId = id_tokens.groupBy(\"id\").agg(count(\"id\").alias(\"allTokensForId\"))\n",
    "\n",
    "    tfds = id_tokens.groupBy(\"id\", \"token\").agg(count(\"id\").alias(\"rawtf\"))\n",
    "    dfds = id_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\"))\n",
    "\n",
    "    # Join the two DataFrames on 'id'\n",
    "    merged_df = tfds.join(allTokensForId, on=\"id\")\n",
    "\n",
    "    # Calculate the ratio of rawtf to allTokensForId\n",
    "    tfds = merged_df.withColumn(\"tf\", col(\"rawtf\") / col(\"allTokensForId\"))\n",
    "\n",
    "    merged_df.show()\n",
    "\n",
    "    # Define the UDF for idf calculation\n",
    "    spark.udf.register(\"calcidfudf\", lambda df: calcidf(doc_count, df), DoubleType())\n",
    "\n",
    "    # Calculate idf and add it as a new column \"idf\"\n",
    "    tokens_idf = dfds.withColumn(\"idf\", expr(\"calcidfudf(df)\"))\n",
    "\n",
    "    # Show the resulting dataframe\n",
    "    tfidfds = tokens_idf.join(tfds, \"token\", \"left\") \\\n",
    "        .withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "\n",
    "    return tfidfds\n",
    "\n",
    "def calcidf(doc_count, df):\n",
    "    # Calculate the tf-idf using natural log\n",
    "    return math.log((doc_count + 1.0) / (df + 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396ff4a5-c2c5-4cee-99bb-e609153b7e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               token| df|\n",
      "+--------------------+---+\n",
      "|  !j@l#y$z%x6x7q8c9z|  2|\n",
      "|                 #gp|  2|\n",
      "|               $_get|  2|\n",
      "|              $_post|  3|\n",
      "|                 %0a|  2|\n",
      "|              %path%|  3|\n",
      "|%programfiles%\\1e...|  2|\n",
      "|               &amp;|  3|\n",
      "|        &quot;public|  2|\n",
      "|          &quot;safe|  2|\n",
      "|             **not**|  2|\n",
      "|        **resolved**|  2|\n",
      "|           **version|  5|\n",
      "|       *.example.com|  2|\n",
      "|               *ctxt|  2|\n",
      "|               *note| 19|\n",
      "|                 *pb|  2|\n",
      "|                 -->|  4|\n",
      "|            --config|  2|\n",
      "|--enable-experime...|  3|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-----------------+-----+--------------+\n",
      "|           id|            token|rawtf|allTokensForId|\n",
      "+-------------+-----------------+-----+--------------+\n",
      "|CVE-2020-0002|             code|    1|            24|\n",
      "|CVE-2020-0003|             user|    1|            23|\n",
      "|CVE-2020-0006|           needed|    2|            29|\n",
      "|CVE-2020-0022|             code|    1|            26|\n",
      "|CVE-2020-0022|        android-9|    1|            26|\n",
      "|CVE-2020-0029|             lead|    1|            22|\n",
      "|CVE-2020-0032|           needed|    2|            23|\n",
      "|CVE-2020-0033|            write|    1|            23|\n",
      "|CVE-2020-0038|        rw_i93.cc|    1|            26|\n",
      "|CVE-2020-0082|            local|    1|            20|\n",
      "|CVE-2020-0082|      interaction|    1|            20|\n",
      "|CVE-2020-0097|      interaction|    1|            24|\n",
      "|CVE-2020-0118|      interaction|    1|            20|\n",
      "|CVE-2020-0138|          missing|    1|            25|\n",
      "|CVE-2020-0165|            local|    1|            26|\n",
      "|CVE-2020-0167|           bounds|    1|            20|\n",
      "|CVE-2020-0173|       eas_mdls.c|    1|            21|\n",
      "|CVE-2020-0195|             user|    1|            21|\n",
      "|CVE-2020-0196|android-10android|    1|            22|\n",
      "|CVE-2020-0199|           needed|    2|            18|\n",
      "+-------------+-----------------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|   token| df|              idf|            id|rawtf|allTokensForId|                  tf|             tf_idf|\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4342|    1|            15| 0.06666666666666667|0.49073229226029474|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4341|    1|            21|0.047619047619047616|0.35052306590021054|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-6364|    1|            36|0.027777777777777776|0.20447178844178948|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4322|    1|            27|0.037037037037037035| 0.2726290512557193|\n",
      "|    10.7| 12|7.360984383904421|CVE-2020-28846|    1|            13| 0.07692307692307693| 0.5662295679926478|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4459|    1|            23|0.043478260869565216|0.32004279930019225|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-6369|    1|            25|                0.04|0.29443937535617687|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4323|    1|            28| 0.03571428571428571| 0.2628922994251579|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-8113|    1|             7| 0.14285714285714285| 1.0515691977006316|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-5358|    1|            28| 0.03571428571428571| 0.2628922994251579|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4327|    1|            21|0.047619047619047616|0.35052306590021054|\n",
      "|    10.7| 12|7.360984383904421| CVE-2020-4413|    1|            27|0.037037037037037035| 0.2726290512557193|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0595|    1|            22|0.045454545454545456|0.32808561546653403|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0540|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0538|    1|            20|                0.05|0.36089417701318743|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-8674|    1|            21|0.047619047619047616| 0.3437087400125594|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0531|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0596|    1|            23|0.043478260869565216| 0.3138210234897282|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0594|    1|            22|0.045454545454545456|0.32808561546653403|\n",
      "|11.12.77| 14|7.217883540263748| CVE-2020-0535|    1|            19| 0.05263157894736842| 0.3798886073823025|\n",
      "+--------+---+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_tokens.show()\n",
    "tfidf_df = tfidf(id_tokens, doc_count)\n",
    "\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37dd189f-0b27-40ab-ae58-f8629e318579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't think I need this anymore\n",
    "# from pyspark.sql import Row\n",
    "# tfidf_df.show()\n",
    "\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('token').alias('words_list'),\n",
    "    collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "# def sort_words(record):\n",
    "#     id, words, tf_idf = record\n",
    "#     sorted_words = sorted(words)\n",
    "#     sorted_tf_idf = [x for _, x in sorted(zip(words, tf_idf))]\n",
    "#     return Row(id=id, words_list=sorted_words, tfidf_list=sorted_tf_idf)\n",
    "\n",
    "# rdd = grouped_tfidf.rdd.map(sort_words)\n",
    "\n",
    "# grouped_tfidf = rdd.toDF()\n",
    "\n",
    "# grouped_tfidf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9334c4-429e-45ec-ae3b-89fcbb32cce8",
   "metadata": {},
   "source": [
    "# Slow Iteration 1\n",
    "```python\n",
    "\"\"\"\n",
    "    Create a sparse vector for each of the documents in the dataset filled with 0's,\n",
    "    replacing the 0 with values that exist for that token on the id.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "token_list = clean_tokens.select('token').collect()\n",
    "\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "def create_sparse_vector(words, tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens\n",
    "    for i, row in enumerate(token_list):\n",
    "        # print(row['token'])\n",
    "        for j, word in enumerate(words):\n",
    "            if row['token'] == word:\n",
    "                # print('match {}'.format(tfidf_list[j]))\n",
    "                values[i] = tfidf_list[j]\n",
    "                break\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "    \n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('words_list', 'tfidf_list', lit(clean_tokens_count)))\n",
    "\n",
    "\n",
    "tfidf_vector_with_sparse.show()\n",
    "# https://spark.apache.org/docs/3.3.0/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse).cache()\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_tfidf_vector.show(truncate=100)\n",
    "\n",
    "print(tfidf_vector_with_sparse.select(\"*\").first())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176b2848-705c-4224-80b4-d1887101cb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15330\n",
      "3925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "token_list = clean_tokens.select('token').collect()\n",
    "# word_list = clean_tokens.select('token').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create a dictionary to map words to indices\n",
    "word_to_index = {word.token: i for i, word in enumerate(token_list)}\n",
    "print(len(word_to_index))\n",
    "\n",
    "index = word_to_index['android']\n",
    "print(index)\n",
    "# print(word_to_index)\n",
    "\n",
    "def create_sparse_vector(words, tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens\n",
    "    for tfidf_idx, word in enumerate(words):\n",
    "        if word in word_to_index:\n",
    "            index = word_to_index[word]\n",
    "            values[index] = tfidf_list[tfidf_idx]\n",
    "    \n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('words_list', 'tfidf_list', lit(clean_tokens_count)))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a13ce51a-897a-463b-9432-75394e98760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize the vectors\n",
    "normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"normFeatures\")\n",
    "normalized = normalizer.transform(final_tfidf_vector).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ddd6018-ea66-41bc-903f-535db9c31c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 13:46:47 WARN DAGScheduler: Broadcasting large task binary with size 1555.8 KiB\n",
      "24/03/23 13:46:47 WARN DAGScheduler: Broadcasting large task binary with size 1555.8 KiB\n",
      "24/03/23 13:46:47 WARN DAGScheduler: Broadcasting large task binary with size 1555.8 KiB\n",
      "24/03/23 13:46:47 WARN DAGScheduler: Broadcasting large task binary with size 1553.7 KiB\n",
      "[Stage 308:==================================================>  (191 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+\n",
      "|           id|          words_list|cosine_sim|\n",
      "+-------------+--------------------+----------+\n",
      "|CVE-2020-0003|[android, interac...|       1.0|\n",
      "|CVE-2020-0236|[android, disclos...| 0.5676376|\n",
      "|CVE-2020-0002|[android, code, i...|0.55887586|\n",
      "|CVE-2020-0008|[android, disclos...| 0.5543941|\n",
      "|CVE-2020-0215|[android, interac...| 0.5533824|\n",
      "+-------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Get a document's features\n",
    "first_doc_features = normalized.filter(col('id') == 'CVE-2020-0003').select('normFeatures').first()[0]\n",
    "\n",
    "\n",
    "# first_doc_features = normalized.first().normFeatures\n",
    "def cosine_similarity(v):\n",
    "    dot_product = float(first_doc_features.dot(v))\n",
    "    norm_product = float(first_doc_features.norm(2)) * float(v.norm(2))\n",
    "    return float(dot_product) / float(norm_product)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "# Compute the cosine similarity and add it as a new column\n",
    "data = normalized.withColumn(\"cosine_sim\", cosine_similarity_udf(col(\"normFeatures\")))\n",
    "\n",
    "# Show the top 5 documents\n",
    "data.sort(col(\"cosine_sim\").desc()).select('id', 'words_list', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922b0e4-863d-4580-a03f-925705c1bcd0",
   "metadata": {},
   "source": [
    "# Top Results:\n",
    "# Search Doc: CVE-2020-0003  \n",
    "In onCreate of InstallStart.java, there is a possible package validation bypass due to a time-of-check time-of-use vulnerability. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is needed for exploitation. Product: Android Versions: Android-8.0 Android ID: A-140195904\n",
    " \n",
    "### CVE-2020-0236\n",
    "In A2DP_GetCodecType of a2dp_codec_config, there is a possible out-of-bounds read due to improper input validation. This could lead to remote information disclosure with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android, Versions: Android-10, Android ID: A-79703353.\"\n",
    "\n",
    "### CVE-2020-0002\n",
    "In ih264d_init_decoder of ih264d_api.c, there is a possible out of bounds write due to a use after free. This could lead to remote code execution with no additional execution privileges needed. User interaction is needed for exploitation Product: Android Versions: Android-8.0, Android-8.1, Android-9, and Android-10 Android ID: A-142602711\n",
    "\n",
    "### CVE-2020-0008\n",
    "In LowEnergyClient::MtuChangedCallback of low_energy_client.cc, there is a possible out of bounds read due to a race condition. This could lead to local information disclosure with no additional execution privileges needed. User interaction is not needed for exploitation. Product: Android Versions: Android-8.0, Android-8.1, Android-9, and Android-10 Android ID: A-142558228\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c39257-bcea-40b7-94cf-b2050b3ca81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb6d6b4f-05ff-4217-9df0-873e02f70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 13:46:52 WARN DAGScheduler: Broadcasting large task binary with size 1555.8 KiB\n",
      "24/03/23 13:46:52 WARN DAGScheduler: Broadcasting large task binary with size 1555.8 KiB\n",
      "24/03/23 13:46:52 WARN DAGScheduler: Broadcasting large task binary with size 1555.8 KiB\n",
      "24/03/23 13:46:52 WARN DAGScheduler: Broadcasting large task binary with size 1553.4 KiB\n",
      "[Stage 336:=============================================>       (170 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+------------------+\n",
      "|           id|          words_list|        cosine_sim|\n",
      "+-------------+--------------------+------------------+\n",
      "|CVE-2020-0003|[android, interac...|0.9999999999999998|\n",
      "|CVE-2020-0236|[android, disclos...|0.5676376436703957|\n",
      "|CVE-2020-0002|[android, code, i...|0.5588758555631687|\n",
      "|CVE-2020-0008|[android, disclos...|0.5543941394005784|\n",
      "|CVE-2020-0215|[android, interac...| 0.553382375146582|\n",
      "+-------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Throw away simply testing performance here.\n",
    "\n",
    "# Get the first document's features\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# first_doc_features = final_tfidf_vector.first().tfidf_features\n",
    "first_doc_features = normalized.filter(col('id') == 'CVE-2020-0003').select('normFeatures').first()[0]\n",
    "\n",
    "# Broadcast the first document's features\n",
    "first_doc_features_bc = sc.broadcast(first_doc_features)\n",
    "\n",
    "\n",
    "def cosine_similarity(v):\n",
    "    return float(first_doc_features_bc.value.dot(v) / (first_doc_features_bc.value.norm(2) * v.norm(2)))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Compute the cosine similarity and add it as a new column\n",
    "data = normalized.withColumn(\"cosine_sim\", cosine_similarity_udf(col(\"tfidf_features\")))\n",
    "data.sort(col(\"cosine_sim\").desc()).select('id', 'words_list', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402679d-b95e-415f-93b5-d72d01fc264b",
   "metadata": {},
   "source": [
    "# IDEAS\n",
    "Already calculated the TF-IDF scores for the documents. Now, letâ€™s create a TF-IDF vector from these scores.\n",
    "\n",
    "Assuming you have a DataFrame named tfidf_vector with columns â€˜idâ€™, â€˜tokenâ€™, â€˜tf_idfâ€™, and â€˜allTokensForIdâ€™, you can proceed as follows:\n",
    "\n",
    "    Group TF-IDF Scores by Document ID:\n",
    "        Group the DataFrame by the â€˜idâ€™ column and aggregate the â€˜tf_idfâ€™ values into a list for each document.\n",
    "        This will give you a list of TF-IDF scores for each document.\n",
    "\n",
    "    Create a Sparse Vector Representation:\n",
    "        Use the SparseVector class from PySpark to create a sparse vector representation for each document.\n",
    "        The vector will have dimensions equal to the total number of unique tokens (terms) in your dataset.\n",
    "        For each document, set the value at the index corresponding to the token to its corresponding TF-IDF score.\n",
    "\n",
    "    Assemble the Sparse Vectors:\n",
    "        Assemble the sparse vectors into a single column using the VectorAssembler.\n",
    "        This will give you a new DataFrame with a column containing the TF-IDF vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e936f122-60fa-4c3f-acc3-ec34140ba8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+------------------+\n",
      "|            id|               words|         rawFeatures|        normFeatures|        cosine_sim|\n",
      "+--------------+--------------------+--------------------+--------------------+------------------+\n",
      "| CVE-2020-0003|[android, interac...|(20000,[392,1662,...|(20000,[392,1662,...|1.0000000000000002|\n",
      "| CVE-2020-0009|[android, corrupt...|(20000,[392,1662,...|(20000,[392,1662,...|0.6982972487551755|\n",
      "| CVE-2020-0215|[android-11, andr...|(20000,[104,392,1...|(20000,[104,392,1...|0.6888747637021418|\n",
      "|CVE-2020-27059|[android, interac...|(20000,[392,2803,...|(20000,[392,2803,...|0.6681531047810613|\n",
      "| CVE-2020-0202|[android, develop...|(20000,[392,1662,...|(20000,[392,1662,...| 0.654653670707977|\n",
      "+--------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# This is just to compare how my vectorization method compares to ML libs \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, Normalizer\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('token').alias('words'))\n",
    "ht = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "featurizedData = ht.transform(grouped_tfidf)\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"rawFeatures\", outputCol=\"normFeatures\")\n",
    "normData = normalizer.transform(featurizedData)\n",
    "\n",
    "first_doc_features = normData.filter(col('id') == 'CVE-2020-0003').select('normFeatures').first()[0]\n",
    "\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Broadcast the first document's features\n",
    "first_doc_features_bc = sc.broadcast(first_doc_features)\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "def cosine_similarity(v):\n",
    "    return float(first_doc_features_bc.value.dot(v) / (first_doc_features_bc.value.norm(2) * v.norm(2)))\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "# Compute cosine similarity\n",
    "result = normData.withColumn(\"cosine_sim\", cosine_similarity_udf(col('normFeatures')))\n",
    "\n",
    "# Get the top similar documents\n",
    "top_similar_docs = result.sort(col(\"cosine_sim\").desc()).limit(5)\n",
    "\n",
    "top_similar_docs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6c381-5054-4c8b-8cdf-acdce01f1f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7131c-23d2-4ba2-a8fb-f579cb838370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "# Create Sparse Vectors\n",
    "def create_sparse_vector(tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens  # Initialize all values to 0.0\n",
    "    for i, tfidf_score in enumerate(tfidf_list):\n",
    "        values[i] = tfidf_score\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('tfidf_list', lit(clean_tokens_count)))\n",
    "\n",
    "# Assemble the Sparse Vectors\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_tfidf_vector.select('id', 'tfidf_features').show(truncate=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaf3fd-3797-4f44-9043-bb762e1ac213",
   "metadata": {},
   "source": [
    "Now that you have the TF-IDF vectors for the documents, here are some useful things to do with the results:\n",
    "\n",
    "    Document Similarity:\n",
    "        Calculate the similarity between documents using cosine similarity or other distance metrics. The closer the vectors, the more similar the documents.\n",
    "        For example, you can find similar documents to a given query document by comparing their TF-IDF vectors.\n",
    "\n",
    "    Topic Modeling:\n",
    "        Apply topic modeling techniques (such as Latent Dirichlet Allocation or Non-Negative Matrix Factorization) to discover underlying topics in your corpus.\n",
    "        Use the TF-IDF vectors as input for these models.\n",
    "\n",
    "    Classification and Clustering:\n",
    "        Train machine learning models (e.g., SVM, Random Forest, or k-means) using the TF-IDF vectors as features.\n",
    "        Classify documents into predefined categories or cluster similar documents together.\n",
    "\n",
    "    Keyword Extraction:\n",
    "        Identify important keywords or phrases within each document based on their TF-IDF scores.\n",
    "        Higher TF-IDF scores indicate more significant terms.\n",
    "\n",
    "    Search and Retrieval:\n",
    "        Use the TF-IDF vectors to build an efficient search index for your documents.\n",
    "        Given a query, retrieve relevant documents based on their similarity to the query.\n",
    "\n",
    "    Visualizations:\n",
    "        Visualize the TF-IDF vectors in lower dimensions using techniques like t-SNE or PCA.\n",
    "        Explore the distribution of documents in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f64e0d-afdb-464d-a680-97665b63017b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"cosine_sim2\", cos_sim_udf(col(\"normFeatures\")))\n",
    "data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80254d-b90f-4df1-95da-41f423b18bdb",
   "metadata": {},
   "source": [
    "# The results:\n",
    "## Results from first iteration - No stop words were removed\n",
    "### CVE-2020-0003 (Search Document)\n",
    "In onCreate of InstallStart.java, there is a possible package validation bypass due to a time-of-check time-of-use vulnerability. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is needed for exploitation. Product: Android Versions: Android-8.0 Android ID: A-140195904\n",
    "\n",
    "### CVE-2020-25162\n",
    "A XPath injection vulnerability in the B. Braun Melsungen AG SpaceCom Version L81/U61 and earlier, and the Data module compactplus Versions A10 and A11 allows unauthenticated remote attackers to access sensitive information and escalate privileges\n",
    "\n",
    "### CVE-2020-24721\n",
    "An issue was discovered in the GAEN (aka Google/Apple Exposure Notifications) protocol through 2020-09-29, as used in COVID-19 applications on Android and iOS. It allows a user to be put in a position where he or she can be coerced into proving or disproving an exposure notification, because of the persistent state of a private framework.\n",
    "\n",
    "There could be a few reasons why the documents getting aren't as similar as I'd like. Here are some things to consider:\n",
    "\n",
    "1. **Quality of your data**: The quality and relevance of the documents in your dataset can greatly affect the results of cosine similarity. If the documents in your dataset are not very similar to your target document to begin with, the top results might not seem very similar.\n",
    "\n",
    "2. **Preprocessing**: How you preprocess your data can also affect the results. This includes things like removing stop words, stemming or lemmatization, and how you handle punctuation and capitalization. You might need to adjust your preprocessing steps to better suit your data.\n",
    "\n",
    "3. **Vectorization**: The method you use to convert your text data into numerical vectors can also have a big impact. You're currently using TF-IDF, which is a good choice for many applications, but there might be other methods that work better for your specific dataset. You could experiment with other methods like Word2Vec, Doc2Vec, or BERT embeddings.\n",
    "\n",
    "4. **Dimensionality**: High-dimensional data can be problematic for cosine similarity due to the curse of dimensionality. You might want to try reducing the dimensionality of your data with techniques like PCA or t-SNE.\n",
    "\n",
    "5. **Thresholding**: You might want to consider applying a threshold to your cosine similarity scores. This means that you only consider documents as \"similar\" if their cosine similarity score is above a certain threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102ee23-5906-4781-95a5-f514033ecd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f82bcc-458e-4599-9b31-bb781f28bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a829a-d5c9-4d6a-be16-8903b6521971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc47cd9-209a-4370-9672-02419a964db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
