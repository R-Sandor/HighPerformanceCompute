{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25839c2-9d92-4789-9dd7-353952d23077",
   "metadata": {},
   "source": [
    "# Volt\n",
    "# Summary \n",
    "In this project the National Vulnerability Database's (NVD) of Common exposures and vulnerabilites (CVES) is levarged\n",
    "to create a tool that is used as a part of the data pipeline to determine how similar CVE's are to one another. \n",
    "\n",
    "The first goal to such a pipeline is to clean the description data, tokenize the data. The second goal is tokenize and read the data that will be used as a training label. \n",
    "## Project Objective: \n",
    "### Create a Pipeline that Can Support Document Similarity and Search\n",
    "\n",
    "\n",
    "# Requirements\n",
    "Required packages:\n",
    "- pyspark\n",
    "- numpy\n",
    "\n",
    "TF-IDF use cases:\n",
    "- LDA\n",
    "- Similarity with\n",
    "1. **Feature Selection**:\n",
    "   - Examine the top TF-IDF terms to identify important features. You can select a subset of these features for further analysis or modeling.\n",
    "   - Consider removing low-TF-IDF terms (common words) that might not contribute significantly to your task.\n",
    "\n",
    "2. **Clustering and Topic Modeling**:\n",
    "   - Apply clustering algorithms (e.g., K-means, DBSCAN) to group similar documents based on their TF-IDF vectors.\n",
    "   - Explore topic modeling techniques (e.g., Latent Dirichlet Allocation, Non-Negative Matrix Factorization) to discover latent topics within your corpus.\n",
    "\n",
    "3. **Document Similarity**:\n",
    "   - Calculate cosine similarity between TF-IDF vectors of different documents. This helps identify similar documents.\n",
    "   - Use similarity scores to recommend related articles, products, or content.\n",
    "\n",
    "4. **Classification and Sentiment Analysis**:\n",
    "   - Train classifiers (e.g., SVM, Random Forest) using TF-IDF features as input. This is useful for tasks like sentiment analysis, spam detection, or document categorization.\n",
    "   - Convert text data into TF-IDF vectors and use them as features for machine learning models.\n",
    "\n",
    "5. **Search and Information Retrieval**:\n",
    "   - Build an inverted index using TF-IDF vectors to create an efficient search engine.\n",
    "   - Retrieve relevant documents based on user queries by ranking them using their TF-IDF scores.\n",
    "\n",
    "6. **Visualizations**:\n",
    "   - Visualize TF-IDF scores using word clouds, scatter plots, or bar charts to gain insights into term importance.\n",
    "   - Plot the distribution of TF-IDF values across the entire dataset.\n",
    "\n",
    "7. **Optimize Hyperparameters**:\n",
    "   - Experiment with different parameters (e.g., n-grams, stop words, max features) in your TF-IDF vectorization process.\n",
    "   - Use cross-validation to find optimal settings.\n",
    "\n",
    "TODO: Find sources to back up these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec292ec-4971-401b-bb8a-df0d79a467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, re\n",
    "import pyspark\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  col, count, countDistinct, concat_ws, desc, explode, expr, lit, udf, split, sum\n",
    "from pyspark.sql.types import DoubleType, FloatType, StringType, ArrayType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import VectorUDT, SparseVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d229f2-1456-48f6-933b-7aa286239fb8",
   "metadata": {},
   "source": [
    "# Download data\n",
    "Handle downloading the data if it doesn't already exist for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abf5b21-76a6-45a8-862b-ab0e82b43e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset if it doesn't already exist. \n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True) \n",
    "fileUrls = [\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2003.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2004.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2005.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2006.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2007.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2008.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2009.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2010.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2011.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-recent.json.zip'\n",
    "    ]\n",
    "\n",
    "# Iterate through each URL\n",
    "for url in fileUrls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    outputfile = os.path.join(data_dir, filename)\n",
    "    checkfile = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(checkfile):\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, outputfile)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "        # Extract the file\n",
    "        with zipfile.ZipFile(outputfile, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "        # Delete the original zip file\n",
    "        os.remove(outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0844e87-acd6-484f-b351-063213f0be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 09:47:30 WARN Utils: Your hostname, sandbox resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface eth0)\n",
      "24/03/20 09:47:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/20 09:47:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CVE_Items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- configurations: struct (nullable = true)\n",
      " |    |    |    |-- CVE_data_version: string (nullable = true)\n",
      " |    |    |    |-- nodes: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- children: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- cpe_match: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- cpe23Uri: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- cpe_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionEndExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionEndIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionStartExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- versionStartIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |    |-- vulnerable: boolean (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- operator: string (nullable = true)\n",
      " |    |    |    |    |    |-- cpe_match: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- cpe23Uri: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- cpe_name: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- versionEndExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionEndIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionStartExcluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- versionStartIncluding: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- vulnerable: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- operator: string (nullable = true)\n",
      " |    |    |-- cve: struct (nullable = true)\n",
      " |    |    |    |-- CVE_data_meta: struct (nullable = true)\n",
      " |    |    |    |    |-- ASSIGNER: string (nullable = true)\n",
      " |    |    |    |    |-- ID: string (nullable = true)\n",
      " |    |    |    |-- data_format: string (nullable = true)\n",
      " |    |    |    |-- data_type: string (nullable = true)\n",
      " |    |    |    |-- data_version: string (nullable = true)\n",
      " |    |    |    |-- description: struct (nullable = true)\n",
      " |    |    |    |    |-- description_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- problemtype: struct (nullable = true)\n",
      " |    |    |    |    |-- problemtype_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- description: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |    |-- lang: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |    |-- value: string (nullable = true)\n",
      " |    |    |    |-- references: struct (nullable = true)\n",
      " |    |    |    |    |-- reference_data: array (nullable = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- refsource: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- tags: array (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- impact: struct (nullable = true)\n",
      " |    |    |    |-- baseMetricV2: struct (nullable = true)\n",
      " |    |    |    |    |-- acInsufInfo: boolean (nullable = true)\n",
      " |    |    |    |    |-- cvssV2: struct (nullable = true)\n",
      " |    |    |    |    |    |-- accessComplexity: string (nullable = true)\n",
      " |    |    |    |    |    |-- accessVector: string (nullable = true)\n",
      " |    |    |    |    |    |-- authentication: string (nullable = true)\n",
      " |    |    |    |    |    |-- availabilityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- baseScore: double (nullable = true)\n",
      " |    |    |    |    |    |-- confidentialityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- integrityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- vectorString: string (nullable = true)\n",
      " |    |    |    |    |    |-- version: string (nullable = true)\n",
      " |    |    |    |    |-- exploitabilityScore: double (nullable = true)\n",
      " |    |    |    |    |-- impactScore: double (nullable = true)\n",
      " |    |    |    |    |-- obtainAllPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- obtainOtherPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- obtainUserPrivilege: boolean (nullable = true)\n",
      " |    |    |    |    |-- severity: string (nullable = true)\n",
      " |    |    |    |    |-- userInteractionRequired: boolean (nullable = true)\n",
      " |    |    |    |-- baseMetricV3: struct (nullable = true)\n",
      " |    |    |    |    |-- cvssV3: struct (nullable = true)\n",
      " |    |    |    |    |    |-- attackComplexity: string (nullable = true)\n",
      " |    |    |    |    |    |-- attackVector: string (nullable = true)\n",
      " |    |    |    |    |    |-- availabilityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- baseScore: double (nullable = true)\n",
      " |    |    |    |    |    |-- baseSeverity: string (nullable = true)\n",
      " |    |    |    |    |    |-- confidentialityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- integrityImpact: string (nullable = true)\n",
      " |    |    |    |    |    |-- privilegesRequired: string (nullable = true)\n",
      " |    |    |    |    |    |-- scope: string (nullable = true)\n",
      " |    |    |    |    |    |-- userInteraction: string (nullable = true)\n",
      " |    |    |    |    |    |-- vectorString: string (nullable = true)\n",
      " |    |    |    |    |    |-- version: string (nullable = true)\n",
      " |    |    |    |    |-- exploitabilityScore: double (nullable = true)\n",
      " |    |    |    |    |-- impactScore: double (nullable = true)\n",
      " |    |    |-- lastModifiedDate: string (nullable = true)\n",
      " |    |    |-- publishedDate: string (nullable = true)\n",
      " |-- CVE_data_format: string (nullable = true)\n",
      " |-- CVE_data_numberOfCVEs: string (nullable = true)\n",
      " |-- CVE_data_timestamp: string (nullable = true)\n",
      " |-- CVE_data_type: string (nullable = true)\n",
      " |-- CVE_data_version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"voltcve\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.default.parallelism\", 8)\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "       .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "# read the data\n",
    "cves = spark.read.option(\"multiline\", \"true\").json(\"data/nvdcve-1.1-2020.json\")\n",
    "\n",
    "# Manipulate the data to be more usable \n",
    "cves.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6172a-4a1e-46f4-8920-697f3793ff3b",
   "metadata": {},
   "source": [
    "# Complex JSON\n",
    "Look at the Schema above, the data is complex and Spark can read the data into a DataFrame. The DataFrame being equvielent to Spark SQL. \n",
    "\n",
    "From the complex schema the data needs to be turned into a more usable format. \n",
    "\n",
    "## Section Objective:\n",
    "- Get Ids\n",
    "- Description Data\n",
    "- Remove unnecessary nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32967fa-907f-4385-86a4-07a81cee8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = cves.select(explode(col(\"CVE_Items\")).alias(\"cves\"))\n",
    "\n",
    "descr_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"),\n",
    "          col(\"cves.cve.description.description_data.value\").alias(\"description\"));\n",
    "\n",
    "descr_df = descr_df.withColumn(\"description_single\", concat_ws(\" \", descr_df[\"description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e1de82-2fce-480c-b534-a40392dc6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 20453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_count = descr_df.selectExpr(\"count(distinct id)\").first()[0]\n",
    "print(\"Number of docs: {}\".format(doc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a2b2a-a08d-4ae5-ac04-68e27baadf8a",
   "metadata": {},
   "source": [
    "# CVE Product Info\n",
    "Below the goal is to get useful information about the CVEs. For example, the type of tool or software that had a vulnerability.\n",
    "- Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "- If it is not used in the application the data may be used in other applications the data later.\n",
    "\n",
    "\n",
    "Example of CPEs: \\\n",
    "cpe:2.3:a:ntp:ntp:4.2.8:p3:*:*:*:*:*:* \\\n",
    "cpe:2.3:o:microsoft:windows_7:-:sp2:*:*:*:*:*:* \\\n",
    "cpe:2.3:a:microsoft:internet_explorer:8.0.6001:beta:*:*:*:*:*:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc9d497-4eab-40fc-a888-62af78b67b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|           id|product|\n",
      "+-------------+-------+\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0001|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0002|android|\n",
      "|CVE-2020-0003|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0004|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0005|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0006|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0007|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0008|android|\n",
      "|CVE-2020-0009|android|\n",
      "|CVE-2020-0010|android|\n",
      "|CVE-2020-0011|android|\n",
      "|CVE-2020-0012|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0014|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0015|android|\n",
      "|CVE-2020-0016|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0017|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "|CVE-2020-0018|android|\n",
      "+-------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now 'exploded' contains individual rows for each 'CVE_Item'\n",
    "# Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "# cpe:<cpe_version>:<part>:<vendor>:<product>:<version>:<update>:<edition>:<language>:<sw_edition>:<target_sw>:<target_hw>:<other>\n",
    "cpe_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"), explode(col(\"cves.configurations.nodes.cpe_match\")[0]).alias(\"cpe\"))\n",
    "\n",
    "cpe_df = cpe_df.select(col(\"id\"), col(\"cpe\").alias(\"cpe\"))\n",
    "cpe_df = cpe_df.select(\"id\", split(cpe_df.cpe.cpe23uri,\":\",-1)[4].alias(\"product\"))\n",
    "\n",
    "# Sample of the data - schema/show\n",
    "cpe_df.printSchema()\n",
    "cpe_df.show(n=50, truncate=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5f30d-ca76-4915-b2a7-fe3fa3c6e0b0",
   "metadata": {},
   "source": [
    "## Uses from the cpe data:\n",
    "- CPE could be a training label.\n",
    "- CPE used in clustering.\n",
    "- Document filtering. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a058d1-e741-479e-9e49-4fedc15f71f8",
   "metadata": {},
   "source": [
    "# Tokenization: \n",
    "## Applying the right tokenization methods:\n",
    "The dataset contains a lot of data the ordinary tokenization may not apply. For example, file names contain puncutation and can leave the token meaningless. \n",
    "An effort was made to preserve all meaninful punction that would describe software or hardware configurations, while removing stop words, and ordinary english punctation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850909ac-b461-45b6-8d55-327f2d71609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------+\n",
      "|           id|                      token|\n",
      "+-------------+---------------------------+\n",
      "|CVE-2020-0001|     getprocessrecordlocked|\n",
      "|CVE-2020-0001|activitymanagerservice.java|\n",
      "|CVE-2020-0001|                   isolated|\n",
      "|CVE-2020-0001|                       apps|\n",
      "|CVE-2020-0001|                    handled|\n",
      "|CVE-2020-0001|                  correctly|\n",
      "|CVE-2020-0001|                       lead|\n",
      "|CVE-2020-0001|                      local|\n",
      "|CVE-2020-0001|                 escalation|\n",
      "|CVE-2020-0001|                  privilege|\n",
      "|CVE-2020-0001|                 additional|\n",
      "|CVE-2020-0001|                  execution|\n",
      "|CVE-2020-0001|                 privileges|\n",
      "|CVE-2020-0001|                     needed|\n",
      "|CVE-2020-0001|                       user|\n",
      "|CVE-2020-0001|                interaction|\n",
      "|CVE-2020-0001|                     needed|\n",
      "|CVE-2020-0001|               exploitation|\n",
      "|CVE-2020-0001|                    product|\n",
      "|CVE-2020-0001|                    android|\n",
      "+-------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def string_cleaner(input_str):\n",
    "    # 1. Replace all \".\" or ':' followed by whitespace with an empty string.\n",
    "    # a. Remove ending periods.\n",
    "    # 2. Remove trademark, rights.\n",
    "    # 3. Grab cotent in parentheses only.\n",
    "    # 4. Remove some punctuation.\n",
    "    cleaned_text = re.sub(r\"[.:,]+\\s+\", \" \", input_str)\n",
    "    # Remove trailing periods\n",
    "    cleaned_text = re.sub(r\"\\.$\", \"\", cleaned_text)\n",
    "    # Remove apostrophes, (TM), (R), parentheses, and double quotes\n",
    "    cleaned_text = re.sub(r\"\\'|\\(TM\\)|\\(R\\)|\\(|\\)|\\\"\", \"\", cleaned_text)\n",
    "    # Convert to lowercase and strip leading/trailing spaces\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "clean_tokens = descr_df.withColumn(\"token\", split(string_cleaner(col(\"description_single\")), \" \" ))\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"token\", outputCol=\"cleanToken\")\n",
    "clean_tokens = stop_words_remover.transform(clean_tokens)\n",
    "clean_tokens = clean_tokens.select(\"id\", explode(\"cleantoken\").alias(\"token\"))\n",
    "clean_tokens = clean_tokens.filter(col(\"token\").isNotNull())\n",
    "\n",
    "# # Show the resulting DataFrame\n",
    "clean_tokens.show(truncate=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a469f-5dca-4ba7-9ed7-c99179b1138d",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "In previous iterations of the application it became apparent that this dataset has its own set of stop words that causes TF-IDF Vectors (show in latter section), \n",
    "to have matches that were not as good as they could because of similar low importance words. \n",
    "\n",
    "Given that this is a dataset about CVEs words like 'attack', 'exploit', 'vulnerab*', don't convey as much meaning.  \n",
    "\n",
    "For example examine the top words after removing common stop words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6855082b-fce6-49e1-8297-c4c9c2d068b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|        token|  df|\n",
      "+-------------+----+\n",
      "|vulnerability|8523|\n",
      "|     attacker|6020|\n",
      "|       allows|5608|\n",
      "|          via|4680|\n",
      "|       remote|3847|\n",
      "|     versions|3713|\n",
      "|         user|3690|\n",
      "|         code|3632|\n",
      "|        issue|3367|\n",
      "|    arbitrary|3345|\n",
      "|       access|3181|\n",
      "|     affected|2809|\n",
      "|      crafted|2628|\n",
      "|        allow|2548|\n",
      "|    attackers|2485|\n",
      "|    execution|2450|\n",
      "|          use|2434|\n",
      "|          may|2407|\n",
      "|         file|2332|\n",
      "|      service|2264|\n",
      "|       exists|2222|\n",
      "|       system|2182|\n",
      "|  information|2104|\n",
      "|      exploit|2091|\n",
      "|         data|2035|\n",
      "|      execute|2023|\n",
      "|   discovered|1897|\n",
      "|      version|1862|\n",
      "|           id|1844|\n",
      "|        prior|1811|\n",
      "|          xss|1796|\n",
      "|          due|1783|\n",
      "|   successful|1778|\n",
      "|       server|1755|\n",
      "|        cause|1736|\n",
      "|       number|1718|\n",
      "|        local|1684|\n",
      "|       memory|1663|\n",
      "|         lead|1621|\n",
      "|       reason|1614|\n",
      "|     rejected|1612|\n",
      "|authenticated|1611|\n",
      "|   privileges|1591|\n",
      "|    malicious|1582|\n",
      "|       denial|1575|\n",
      "|        notes|1567|\n",
      "|    candidate|1560|\n",
      "|   consultids|1548|\n",
      "|         none|1507|\n",
      "|          cna|1506|\n",
      "+-------------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clean_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\")).orderBy(desc(\"df\")).show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca5c75-1450-4b1e-ba8b-0978f08c3368",
   "metadata": {},
   "source": [
    "Lets resolve some of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4575e416-304e-4130-b440-7c6a37d8801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 229:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+\n",
      "|          token|  df|\n",
      "+---------------+----+\n",
      "|         remote|3847|\n",
      "|           code|3632|\n",
      "|          issue|3367|\n",
      "|      arbitrary|3345|\n",
      "|         access|3181|\n",
      "|        crafted|2628|\n",
      "|      execution|2450|\n",
      "|           file|2332|\n",
      "|        service|2264|\n",
      "|         system|2182|\n",
      "|    information|2104|\n",
      "|           data|2035|\n",
      "|        execute|2023|\n",
      "|          prior|1811|\n",
      "|            xss|1796|\n",
      "|     successful|1778|\n",
      "|         server|1755|\n",
      "|         number|1718|\n",
      "|          local|1684|\n",
      "|         memory|1663|\n",
      "|           lead|1621|\n",
      "|         reason|1614|\n",
      "|       rejected|1612|\n",
      "|  authenticated|1611|\n",
      "|     privileges|1591|\n",
      "|         denial|1575|\n",
      "|          notes|1567|\n",
      "|      candidate|1560|\n",
      "|     consultids|1548|\n",
      "|           none|1507|\n",
      "|            cna|1506|\n",
      "|           2020|1503|\n",
      "|unauthenticated|1469|\n",
      "|          users|1464|\n",
      "|      privilege|1450|\n",
      "|            web|1402|\n",
      "|      component|1400|\n",
      "|      scripting|1321|\n",
      "|        network|1318|\n",
      "|         result|1316|\n",
      "|           read|1294|\n",
      "|        request|1257|\n",
      "|      injection|1215|\n",
      "|    application|1209|\n",
      "|     vulnerable|1203|\n",
      "|     validation|1199|\n",
      "|       security|1194|\n",
      "|        windows|1167|\n",
      "|          files|1158|\n",
      "|      parameter|1121|\n",
      "+---------------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stopwordslist=['attack', 'attacks', 'attacker', 'attackers',  'vulnerability', 'due', 'may', '1','exploit', 'affects', 'affected', 'exists', 'version', 'versions', 'id' ]\n",
    "clean_tokens = clean_tokens.filter(~col(\"token\").isin(*stopwordslist))\n",
    "\n",
    "clean_tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\")).orderBy(desc(\"df\")).show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a2f8695-9403-4b47-a7ab-0d39a08edafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tokens, doc_count):\n",
    "    allTokensForId = tokens.groupBy(\"id\").agg(count(\"id\").alias(\"allTokensForId\"))\n",
    "\n",
    "    tfds = tokens.groupBy(\"id\", \"token\").agg(count(\"id\").alias(\"rawtf\"))\n",
    "    dfds = tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\"))\n",
    "\n",
    "    # Join the two DataFrames on 'id'\n",
    "    merged_df = tfds.join(allTokensForId, on=\"id\")\n",
    "\n",
    "    # Calculate the ratio of rawtf to allTokensForId\n",
    "    tfds = merged_df.withColumn(\"tf\", col(\"rawtf\") / col(\"allTokensForId\"))\n",
    "    \n",
    "\n",
    "    merged_df.show()\n",
    "\n",
    "    # Define the UDF for idf calculation\n",
    "    spark.udf.register(\"calcidfudf\", lambda df: calcidf(doc_count, df), DoubleType())\n",
    "\n",
    "    # Calculate idf and add it as a new column \"idf\"\n",
    "    tokens_idf = dfds.withColumn(\"idf\", expr(\"calcidfudf(df)\"))\n",
    "\n",
    "    # Show the resulting dataframe\n",
    "    tfidfds = tokens_idf.join(tfds, \"token\", \"left\") \\\n",
    "        .withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "\n",
    "    return tfidfds\n",
    "\n",
    "def calcidf(doc_count, df):\n",
    "    # Calculate the tf-idf using natural log\n",
    "    return math.log((doc_count + 1.0) / (df + 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "396ff4a5-c2c5-4cee-99bb-e609153b7e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+--------------+\n",
      "|           id|               token|rawtf|allTokensForId|\n",
      "+-------------+--------------------+-----+--------------+\n",
      "|CVE-2020-0002|                code|    1|            24|\n",
      "|CVE-2020-0005|         a-141552859|    1|            24|\n",
      "|CVE-2020-0006|              needed|    2|            29|\n",
      "|CVE-2020-0022|                code|    1|            25|\n",
      "|CVE-2020-0022|           android-9|    1|            25|\n",
      "|CVE-2020-0029|                lead|    1|            22|\n",
      "|CVE-2020-0032|              needed|    2|            24|\n",
      "|CVE-2020-0033|               write|    1|            23|\n",
      "|CVE-2020-0038|           rw_i93.cc|    1|            25|\n",
      "|CVE-2020-0082|               local|    1|            22|\n",
      "|CVE-2020-0109|         a-148059175|    1|            23|\n",
      "|CVE-2020-0110|               psi.c|    1|            24|\n",
      "|CVE-2020-0138|             missing|    1|            27|\n",
      "|CVE-2020-0156|         a-139736127|    1|            20|\n",
      "|CVE-2020-0165|               local|    1|            24|\n",
      "|CVE-2020-0167|              bounds|    1|            20|\n",
      "|CVE-2020-0173|          eas_mdls.c|    1|            21|\n",
      "|CVE-2020-0196|   android-10android|    1|            23|\n",
      "|CVE-2020-0199|              needed|    2|            19|\n",
      "|CVE-2020-0203|exploitation.product|    1|            22|\n",
      "+-------------+--------------------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 10:11:43 WARN SimpleFunctionRegistry: The function calcidfudf replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|token|  df|              idf|            id|rawtf|allTokensForId|                  tf|             tf_idf|\n",
      "+-----+----+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "|input|1104|2.918333127414105| CVE-2020-4059|    1|            34|0.029411764705882353|0.08583332727688545|\n",
      "|input|1104|2.918333127414105|CVE-2020-29624|    1|            43|0.023255813953488372| 0.0678682122654443|\n",
      "|input|1104|2.918333127414105|CVE-2020-24753|    1|            42|0.023809523809523808|0.06948412208128821|\n",
      "|input|1104|2.918333127414105|CVE-2020-17479|    1|            12| 0.08333333333333333|0.24319442728450874|\n",
      "|input|1104|2.918333127414105|CVE-2020-12816|    1|            16|              0.0625|0.18239582046338157|\n",
      "|input|1104|2.918333127414105| CVE-2020-7356|    1|            29|0.034482758620689655|0.10063217680738293|\n",
      "|input|1104|2.918333127414105| CVE-2020-6327|    1|            26|0.038461538461538464|0.11224358182361943|\n",
      "|input|1104|2.918333127414105| CVE-2020-0535|    1|            16|              0.0625|0.18239582046338157|\n",
      "|input|1104|2.918333127414105| CVE-2020-5985|    1|            23|0.043478260869565216|0.12688404901800457|\n",
      "|input|1104|2.918333127414105| CVE-2020-3953|    1|            13| 0.07692307692307693|0.22448716364723886|\n",
      "|input|1104|2.918333127414105| CVE-2020-3218|    1|            40|               0.025|0.07295832818535262|\n",
      "|input|1104|2.918333127414105|CVE-2020-28860|    1|            17|0.058823529411764705| 0.1716666545537709|\n",
      "|input|1104|2.918333127414105|CVE-2020-16158|    1|            15| 0.06666666666666667|  0.194555541827607|\n",
      "|input|1104|2.918333127414105|CVE-2020-15792|    1|            20|                0.05|0.14591665637070525|\n",
      "|input|1104|2.918333127414105|CVE-2020-15671|    1|            21|0.047619047619047616|0.13896824416257642|\n",
      "|input|1104|2.918333127414105| CVE-2020-7740|    1|            18| 0.05555555555555555| 0.1621296181896725|\n",
      "|input|1104|2.918333127414105| CVE-2020-6375|    1|            30| 0.03333333333333333| 0.0972777709138035|\n",
      "|input|1104|2.918333127414105|CVE-2020-29651|    1|            18| 0.05555555555555555| 0.1621296181896725|\n",
      "|input|1104|2.918333127414105|CVE-2020-28502|    1|            19| 0.05263157894736842|0.15359648039021606|\n",
      "|input|1104|2.918333127414105|CVE-2020-24313|    1|            28| 0.03571428571428571|0.10422618312193233|\n",
      "+-----+----+-----------------+--------------+-----+--------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_df = tfidf(clean_tokens, doc_count)\n",
    "\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402679d-b95e-415f-93b5-d72d01fc264b",
   "metadata": {},
   "source": [
    "Already calculated the TF-IDF scores for the documents. Now, let’s create a TF-IDF vector from these scores.\n",
    "\n",
    "Assuming you have a DataFrame named tfidf_vector with columns ‘id’, ‘token’, ‘tf_idf’, and ‘allTokensForId’, you can proceed as follows:\n",
    "\n",
    "    Group TF-IDF Scores by Document ID:\n",
    "        Group the DataFrame by the ‘id’ column and aggregate the ‘tf_idf’ values into a list for each document.\n",
    "        This will give you a list of TF-IDF scores for each document.\n",
    "\n",
    "    Create a Sparse Vector Representation:\n",
    "        Use the SparseVector class from PySpark to create a sparse vector representation for each document.\n",
    "        The vector will have dimensions equal to the total number of unique tokens (terms) in your dataset.\n",
    "        For each document, set the value at the index corresponding to the token to its corresponding TF-IDF score.\n",
    "\n",
    "    Assemble the Sparse Vectors:\n",
    "        Assemble the sparse vectors into a single column using the VectorAssembler.\n",
    "        This will give you a new DataFrame with a column containing the TF-IDF vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97b7131c-23d2-4ba2-a8fb-f579cb838370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 287:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------------------------------------------------+\n",
      "|           id|                                                                                      tfidf_features|\n",
      "+-------------+----------------------------------------------------------------------------------------------------+\n",
      "|CVE-2020-0003|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],[0.10853969124258678,0.377603255007...|\n",
      "|CVE-2020-0006|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[0.14423898460539572,0....|\n",
      "|CVE-2020-0009|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.08608320339929297,0.449...|\n",
      "|CVE-2020-0012|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.2771496520398574,0.1134733135717952...|\n",
      "|CVE-2020-0028|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],[0.13411453699590936,0.215135313203...|\n",
      "|CVE-2020-0031|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],[0.12293832557958358,0.104017204...|\n",
      "|CVE-2020-0035|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.11802079255640024,0.099...|\n",
      "|CVE-2020-0036|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],[0.104017204107479,0.07752785551...|\n",
      "|CVE-2020-0042|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.2771496520398574,0.1341145369959093...|\n",
      "|CVE-2020-0044|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.2771496520398574,0.1341145369959093...|\n",
      "|CVE-2020-0050|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],[0.12688404901800457,0.108539691...|\n",
      "|CVE-2020-0052|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],[0.104017204107479,0.19539549077233...|\n",
      "|CVE-2020-0061|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.2540538477032026,0.1229383...|\n",
      "|CVE-2020-0063|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],[0.2771496520398574,0.1134733135717...|\n",
      "|CVE-2020-0064|(40640,[0,1,2,3,4,5,6,7,8],[0.7249707066337558,0.48249304806491766,0.3383944065755503,0.496260914...|\n",
      "|CVE-2020-0067|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.12828347017000025,0.108539691242586...|\n",
      "|CVE-2020-0068|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18],[0.13411453699590936,0.11347331357179527,...|\n",
      "|CVE-2020-0073|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],[0.06912478836579025,0.187579671...|\n",
      "|CVE-2020-0076|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.24389169379507455,0.099856...|\n",
      "|CVE-2020-0078|(40640,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.11347331357179527,0.213158717206182...|\n",
      "+-------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "# Create Sparse Vectors\n",
    "def create_sparse_vector(tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens  # Initialize all values to 0.0\n",
    "    for i, tfidf_score in enumerate(tfidf_list):\n",
    "        values[i] = tfidf_score\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "num_tokens = len(tfidf_df.select('token').distinct().collect())\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('tfidf_list', lit(num_tokens)))\n",
    "\n",
    "# Assemble the Sparse Vectors\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_tfidf_vector.select('id', 'tfidf_features').show(truncate=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaf3fd-3797-4f44-9043-bb762e1ac213",
   "metadata": {},
   "source": [
    "Now that you have the TF-IDF vectors for the documents, here are some useful things to do with the results:\n",
    "\n",
    "    Document Similarity:\n",
    "        Calculate the similarity between documents using cosine similarity or other distance metrics. The closer the vectors, the more similar the documents.\n",
    "        For example, you can find similar documents to a given query document by comparing their TF-IDF vectors.\n",
    "\n",
    "    Topic Modeling:\n",
    "        Apply topic modeling techniques (such as Latent Dirichlet Allocation or Non-Negative Matrix Factorization) to discover underlying topics in your corpus.\n",
    "        Use the TF-IDF vectors as input for these models.\n",
    "\n",
    "    Classification and Clustering:\n",
    "        Train machine learning models (e.g., SVM, Random Forest, or k-means) using the TF-IDF vectors as features.\n",
    "        Classify documents into predefined categories or cluster similar documents together.\n",
    "\n",
    "    Keyword Extraction:\n",
    "        Identify important keywords or phrases within each document based on their TF-IDF scores.\n",
    "        Higher TF-IDF scores indicate more significant terms.\n",
    "\n",
    "    Search and Retrieval:\n",
    "        Use the TF-IDF vectors to build an efficient search index for your documents.\n",
    "        Given a query, retrieve relevant documents based on their similarity to the query.\n",
    "\n",
    "    Visualizations:\n",
    "        Visualize the TF-IDF vectors in lower dimensions using techniques like t-SNE or PCA.\n",
    "        Explore the distribution of documents in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e097ca7a-c628-4d7f-a655-76d14c28d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize the vectors\n",
    "normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"normFeatures\")\n",
    "data = normalizer.transform(final_tfidf_vector)\n",
    "\n",
    "# Get the first document's features\n",
    "first_doc_features = data.first().tfidf_features\n",
    "def cosine_similarity(v):\n",
    "    dot_product = float(first_doc_features.dot(v))\n",
    "    norm_product = float(first_doc_features.norm(2)) * float(v.norm(2))\n",
    "    return float(dot_product) / float(norm_product)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "\n",
    "# Compute the cosine similarity and add it as a new column\n",
    "data = data.withColumn(\"cosine_sim\", cosine_similarity_udf(col(\"normFeatures\")))\n",
    "\n",
    "# Show the top 5 documents\n",
    "#data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aed756f2-a348-4300-ba28-902b6aa5bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-15.0.2-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /home/rsandor/.local/lib/python3.9/site-packages (from pyarrow) (1.23.5)\n",
      "Downloading pyarrow-15.0.2-cp39-cp39-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: ntp 1.2.0.2021-06-17T05-15-04Z has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of ntp or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-15.0.2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.pandas.functions import pandas_udf\n",
    "import pandas as pd\n",
    "# !pip install pyarrow\n",
    "# make sure arrow is actually used\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\n",
    "\n",
    "def cos_sim2(vec: pd.Series) -> pd.Series:\n",
    "    value_norm = np.linalg.norm(value)\n",
    "    cs_value = vec.apply(lambda v: np.dot(value, v) / (np.linalg.norm(v) * value_norm))\n",
    "    return cs_value.replace(np.inf, np.nan)\n",
    "\n",
    "cos_sim_udf = pandas_udf(cos_sim2, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67f64e0d-afdb-464d-a680-97665b63017b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 13:05:46 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "[Stage 351:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|            id|cosine_sim|\n",
      "+--------------+----------+\n",
      "| CVE-2020-0003|       1.0|\n",
      "|CVE-2020-18475| 0.9601347|\n",
      "|CVE-2020-15007|0.95723075|\n",
      "|CVE-2020-36034| 0.9530993|\n",
      "| CVE-2020-0218| 0.9470317|\n",
      "+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.withColumn(\"cosine_sim2\", cos_sim_udf(col(\"normFeatures\")))\n",
    "data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80254d-b90f-4df1-95da-41f423b18bdb",
   "metadata": {},
   "source": [
    "# The results:\n",
    "## Results from first iteration - No stop words were removed\n",
    "### CVE-2020-0003 (Search Document)\n",
    "In onCreate of InstallStart.java, there is a possible package validation bypass due to a time-of-check time-of-use vulnerability. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is needed for exploitation. Product: Android Versions: Android-8.0 Android ID: A-140195904\n",
    "\n",
    "### CVE-2020-25162\n",
    "A XPath injection vulnerability in the B. Braun Melsungen AG SpaceCom Version L81/U61 and earlier, and the Data module compactplus Versions A10 and A11 allows unauthenticated remote attackers to access sensitive information and escalate privileges\n",
    "\n",
    "### CVE-2020-24721\n",
    "An issue was discovered in the GAEN (aka Google/Apple Exposure Notifications) protocol through 2020-09-29, as used in COVID-19 applications on Android and iOS. It allows a user to be put in a position where he or she can be coerced into proving or disproving an exposure notification, because of the persistent state of a private framework.\n",
    "\n",
    "There could be a few reasons why the documents getting aren't as similar as I'd like. Here are some things to consider:\n",
    "\n",
    "1. **Quality of your data**: The quality and relevance of the documents in your dataset can greatly affect the results of cosine similarity. If the documents in your dataset are not very similar to your target document to begin with, the top results might not seem very similar.\n",
    "\n",
    "2. **Preprocessing**: How you preprocess your data can also affect the results. This includes things like removing stop words, stemming or lemmatization, and how you handle punctuation and capitalization. You might need to adjust your preprocessing steps to better suit your data.\n",
    "\n",
    "3. **Vectorization**: The method you use to convert your text data into numerical vectors can also have a big impact. You're currently using TF-IDF, which is a good choice for many applications, but there might be other methods that work better for your specific dataset. You could experiment with other methods like Word2Vec, Doc2Vec, or BERT embeddings.\n",
    "\n",
    "4. **Dimensionality**: High-dimensional data can be problematic for cosine similarity due to the curse of dimensionality. You might want to try reducing the dimensionality of your data with techniques like PCA or t-SNE.\n",
    "\n",
    "5. **Thresholding**: You might want to consider applying a threshold to your cosine similarity scores. This means that you only consider documents as \"similar\" if their cosine similarity score is above a certain threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102ee23-5906-4781-95a5-f514033ecd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f82bcc-458e-4599-9b31-bb781f28bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a829a-d5c9-4d6a-be16-8903b6521971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc47cd9-209a-4370-9672-02419a964db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
