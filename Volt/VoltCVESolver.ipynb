{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25839c2-9d92-4789-9dd7-353952d23077",
   "metadata": {},
   "source": [
    "# ReadMe\n",
    "The objective of this project is to read the National Vulnerability Database's (NVD) of Common exposures and vulnerabilites (CVES) and create a tool \n",
    "that is used as a part of the data pipeline to determine how similar CVE's are to one another. \n",
    "\n",
    "The first goal to such a pipeline is to clean the description data, tokenize the data. The second goal is tokenize and read the data that will be used as a training label. \n",
    "\n",
    "# Requirements\n",
    "Required packages:\n",
    "- pyspark\n",
    "- numpy\n",
    "\n",
    "TF-IDF use cases:\n",
    "- LDA\n",
    "- Similarity with\n",
    "1. **Feature Selection**:\n",
    "   - Examine the top TF-IDF terms to identify important features. You can select a subset of these features for further analysis or modeling.\n",
    "   - Consider removing low-TF-IDF terms (common words) that might not contribute significantly to your task.\n",
    "\n",
    "2. **Clustering and Topic Modeling**:\n",
    "   - Apply clustering algorithms (e.g., K-means, DBSCAN) to group similar documents based on their TF-IDF vectors.\n",
    "   - Explore topic modeling techniques (e.g., Latent Dirichlet Allocation, Non-Negative Matrix Factorization) to discover latent topics within your corpus.\n",
    "\n",
    "3. **Document Similarity**:\n",
    "   - Calculate cosine similarity between TF-IDF vectors of different documents. This helps identify similar documents.\n",
    "   - Use similarity scores to recommend related articles, products, or content.\n",
    "\n",
    "4. **Classification and Sentiment Analysis**:\n",
    "   - Train classifiers (e.g., SVM, Random Forest) using TF-IDF features as input. This is useful for tasks like sentiment analysis, spam detection, or document categorization.\n",
    "   - Convert text data into TF-IDF vectors and use them as features for machine learning models.\n",
    "\n",
    "5. **Search and Information Retrieval**:\n",
    "   - Build an inverted index using TF-IDF vectors to create an efficient search engine.\n",
    "   - Retrieve relevant documents based on user queries by ranking them using their TF-IDF scores.\n",
    "\n",
    "6. **Visualizations**:\n",
    "   - Visualize TF-IDF scores using word clouds, scatter plots, or bar charts to gain insights into term importance.\n",
    "   - Plot the distribution of TF-IDF values across the entire dataset.\n",
    "\n",
    "7. **Optimize Hyperparameters**:\n",
    "   - Experiment with different parameters (e.g., n-grams, stop words, max features) in your TF-IDF vectorization process.\n",
    "   - Use cross-validation to find optimal settings.\n",
    "\n",
    "TODO: Find sources to back up these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec292ec-4971-401b-bb8a-df0d79a467bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, re\n",
    "import pyspark\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import  col, count, countDistinct, concat_ws, explode, expr, lit, udf, split, sum\n",
    "from pyspark.sql.types import DoubleType, FloatType, StringType, ArrayType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import VectorUDT, SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abf5b21-76a6-45a8-862b-ab0e82b43e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset if it doesn't already exist. \n",
    "data_dir = \"data\"\n",
    "os.makedirs(data_dir, exist_ok=True) \n",
    "fileUrls = [\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2002.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2003.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2004.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2005.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2006.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2007.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2008.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2009.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2010.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2011.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2012.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2013.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2014.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2015.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2016.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2017.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2018.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2019.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2022.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2023.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2024.json.zip',\n",
    "        'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-recent.json.zip'\n",
    "    ]\n",
    "\n",
    "# Iterate through each URL\n",
    "for url in fileUrls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    outputfile = os.path.join(data_dir, filename)\n",
    "    checkfile = os.path.join(data_dir, os.path.splitext(filename)[0])\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(checkfile):\n",
    "        # Download the file\n",
    "        urllib.request.urlretrieve(url, outputfile)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "        # Extract the file\n",
    "        with zipfile.ZipFile(outputfile, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "        # Delete the original zip file\n",
    "        os.remove(outputfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a35d5c-291b-44b8-b51d-71ad35bf0f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 07:59:08 WARN Utils: Your hostname, sandbox resolves to a loopback address: 127.0.0.1; using 192.168.0.14 instead (on interface eth0)\n",
      "24/03/19 07:59:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 07:59:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|           id|       label|\n",
      "+-------------+------------+\n",
      "|CVE-2020-0001|     android|\n",
      "|CVE-2020-0002|     android|\n",
      "|CVE-2020-0003|     android|\n",
      "|CVE-2020-0004|     android|\n",
      "|CVE-2020-0005|     android|\n",
      "|CVE-2020-0006|     android|\n",
      "|CVE-2020-0007|     android|\n",
      "|CVE-2020-0008|     android|\n",
      "|CVE-2020-0009|     android|\n",
      "|CVE-2020-0009|debian_linux|\n",
      "|CVE-2020-0010|     android|\n",
      "|CVE-2020-0011|     android|\n",
      "|CVE-2020-0012|     android|\n",
      "|CVE-2020-0014|     android|\n",
      "|CVE-2020-0015|     android|\n",
      "|CVE-2020-0016|     android|\n",
      "|CVE-2020-0017|     android|\n",
      "|CVE-2020-0018|     android|\n",
      "|CVE-2020-0019|     android|\n",
      "|CVE-2020-0020|     android|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 20453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"voltcve\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.default.parallelism\", 8)\n",
    "        .config(\"spark.driver.memory\", \"25g\") \\\n",
    "       .config(\"spark.executor.memory\", \"10g\") \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "# read the data\n",
    "cves = spark.read.option(\"multiline\", \"true\").json(\"data/nvdcve-1.1-2020.json\")\n",
    "\n",
    "# Manipulate the data to be more usable \n",
    "exploded = cves.select(explode(col(\"CVE_Items\")).alias(\"cves\"))\n",
    "\n",
    "\n",
    "# Now 'exploded' contains individual rows for each 'CVE_Item'\n",
    "# Scheme format: https://en.wikipedia.org/wiki/Common_Platform_Enumeration\n",
    "# cpe:<cpe_version>:<part>:<vendor>:<product>:<version>:<update>:<edition>:<language>:<sw_edition>:<target_sw>:<target_hw>:<other>\n",
    "cpe_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"), explode(col(\"cves.configurations.nodes.cpe_match\")).alias(\"cpe\"))\n",
    "\n",
    "# Take just the first row to train. The CPE23Uri will contain a lot of repeated data\n",
    "# As each version is given a row.\n",
    "cpe_df = cpe_df.select(col(\"id\"), col(\"cpe\")[0].alias(\"cpe\"))\n",
    "cpe_df = cpe_df.select(\"id\", split(cpe_df.cpe.cpe23uri,\":\",-1)[4].alias(\"label\"))\n",
    "\n",
    "cpe_df.show(truncate=200)\n",
    "cpe_df.printSchema()\n",
    "\n",
    "descr_df = exploded.select(col(\"cves.cve.CVE_data_meta.ID\").alias(\"id\"),\n",
    "          col(\"cves.cve.description.description_data.value\").alias(\"description\"));\n",
    "\n",
    "descr_df = descr_df.withColumn(\"description_single\", concat_ws(\" \", descr_df[\"description\"]))\n",
    "\n",
    "\n",
    "doc_count = descr_df.selectExpr(\"count(distinct id)\").first()[0]\n",
    "print(\"Number of docs: {}\".format(doc_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5f30d-ca76-4915-b2a7-fe3fa3c6e0b0",
   "metadata": {},
   "source": [
    "## Questions from the data:\n",
    "- Should CPE be trained on the same data with multiple labels?\n",
    "- How much data is enough to adequately train a word2vec model?\n",
    "\n",
    "## Tokenization: \n",
    "### Applying the right tokenization methods:\n",
    "The dataset contains a lot of data the ordinary tokenization may not apply. For example, file names contain puncutation and can leave the token meaningless. \n",
    "An effort was made to preserve all meaninful punction that would describe software or hardware configurations, while removing stop words, and ordinary english punctation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850909ac-b461-45b6-8d55-327f2d71609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------+\n",
      "|           id|                      token|\n",
      "+-------------+---------------------------+\n",
      "|CVE-2020-0001|     getprocessrecordlocked|\n",
      "|CVE-2020-0001|activitymanagerservice.java|\n",
      "|CVE-2020-0001|                   isolated|\n",
      "|CVE-2020-0001|                       apps|\n",
      "|CVE-2020-0001|                    handled|\n",
      "|CVE-2020-0001|                  correctly|\n",
      "|CVE-2020-0001|                       lead|\n",
      "|CVE-2020-0001|                      local|\n",
      "|CVE-2020-0001|                 escalation|\n",
      "|CVE-2020-0001|                  privilege|\n",
      "|CVE-2020-0001|                 additional|\n",
      "|CVE-2020-0001|                  execution|\n",
      "|CVE-2020-0001|                 privileges|\n",
      "|CVE-2020-0001|                     needed|\n",
      "|CVE-2020-0001|                       user|\n",
      "|CVE-2020-0001|                interaction|\n",
      "|CVE-2020-0001|                     needed|\n",
      "|CVE-2020-0001|               exploitation|\n",
      "|CVE-2020-0001|                    product|\n",
      "|CVE-2020-0001|                    android|\n",
      "+-------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def string_cleaner(input_str):\n",
    "    # 1. Replace all \".\" or ':' followed by whitespace with an empty string.\n",
    "    # a. Remove ending periods.\n",
    "    # 2. Remove trademark, rights.\n",
    "    # 3. Grab cotent in parentheses only.\n",
    "    # 4. Remove some punctuation.\n",
    "    cleaned_text = re.sub(r\"[.:,]+\\s+\", \" \", input_str)\n",
    "    # Remove trailing periods\n",
    "    cleaned_text = re.sub(r\"\\.$\", \"\", cleaned_text)\n",
    "    # Remove apostrophes, (TM), (R), parentheses, and double quotes\n",
    "    cleaned_text = re.sub(r\"\\'|\\(TM\\)|\\(R\\)|\\(|\\)|\\\"\", \"\", cleaned_text)\n",
    "    # Convert to lowercase and strip leading/trailing spaces\n",
    "    cleaned_text = cleaned_text.strip().lower()\n",
    "    return cleaned_text\n",
    "\n",
    "clean_tokens = descr_df.withColumn(\"token\", split(string_cleaner(col(\"description_single\")), \" \" ))\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"token\", outputCol=\"cleanToken\")\n",
    "clean_tokens = stop_words_remover.transform(clean_tokens)\n",
    "clean_tokens = clean_tokens.select(\"id\", explode(\"cleantoken\").alias(\"token\"))\n",
    "clean_tokens = clean_tokens.filter(col(\"token\").isNotNull())\n",
    "\n",
    "# # Show the resulting DataFrame\n",
    "clean_tokens.show(truncate=200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a2f8695-9403-4b47-a7ab-0d39a08edafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(tokens, doc_count):\n",
    "    allTokensForId = tokens.groupBy(\"id\").agg(count(\"id\").alias(\"allTokensForId\"))\n",
    "\n",
    "    tfds = tokens.groupBy(\"id\", \"token\").agg(count(\"id\").alias(\"rawtf\"))\n",
    "    dfds = tokens.groupBy(\"token\").agg(countDistinct(\"id\").alias(\"df\"))\n",
    "\n",
    "    # Join the two DataFrames on 'id'\n",
    "    merged_df = tfds.join(allTokensForId, on=\"id\")\n",
    "\n",
    "    # Calculate the ratio of rawtf to allTokensForId\n",
    "    tfds = merged_df.withColumn(\"tf\", col(\"rawtf\") / col(\"allTokensForId\"))\n",
    "    \n",
    "\n",
    "    merged_df.show()\n",
    "\n",
    "    # Define the UDF for idf calculation\n",
    "    spark.udf.register(\"calcidfudf\", lambda df: calcidf(doc_count, df), DoubleType())\n",
    "\n",
    "    # Calculate idf and add it as a new column \"idf\"\n",
    "    tokens_idf = dfds.withColumn(\"idf\", expr(\"calcidfudf(df)\"))\n",
    "\n",
    "    # Show the resulting dataframe\n",
    "    tfidfds = tokens_idf.join(tfds, \"token\", \"left\") \\\n",
    "        .withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "\n",
    "    return tfidfds\n",
    "\n",
    "def calcidf(doc_count, df):\n",
    "    # Calculate the tf-idf using natural log\n",
    "    return math.log((doc_count + 1.0) / (df + 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396ff4a5-c2c5-4cee-99bb-e609153b7e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+--------------+\n",
      "|           id|      token|rawtf|allTokensForId|\n",
      "+-------------+-----------+-----+--------------+\n",
      "|CVE-2020-0002|       code|    1|            30|\n",
      "|CVE-2020-0003|       user|    1|            29|\n",
      "|CVE-2020-0005|a-141552859|    1|            28|\n",
      "|CVE-2020-0006|     needed|    2|            34|\n",
      "|CVE-2020-0022|       code|    1|            29|\n",
      "|CVE-2020-0022|  android-9|    1|            29|\n",
      "|CVE-2020-0029|       lead|    1|            25|\n",
      "|CVE-2020-0032|     needed|    2|            28|\n",
      "|CVE-2020-0033|      write|    1|            27|\n",
      "|CVE-2020-0038|  rw_i93.cc|    1|            29|\n",
      "|CVE-2020-0082|      local|    1|            26|\n",
      "|CVE-2020-0082|interaction|    1|            26|\n",
      "|CVE-2020-0097|interaction|    1|            27|\n",
      "|CVE-2020-0109|a-148059175|    1|            26|\n",
      "|CVE-2020-0110|      psi.c|    1|            28|\n",
      "|CVE-2020-0118|interaction|    1|            25|\n",
      "|CVE-2020-0138|    missing|    1|            31|\n",
      "|CVE-2020-0156|a-139736127|    1|            25|\n",
      "|CVE-2020-0165|      local|    1|            29|\n",
      "|CVE-2020-0167|     bounds|    1|            24|\n",
      "+-------------+-----------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------------+--------------+-----+--------------+--------------------+--------------------+\n",
      "|      token| df|               idf|            id|rawtf|allTokensForId|                  tf|              tf_idf|\n",
      "+-----------+---+------------------+--------------+-----+--------------+--------------------+--------------------+\n",
      "|interaction|990|3.0272192070359702| CVE-2020-9745|    1|            37| 0.02702702702702703|  0.0818167353252965|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-2880|    1|            85|0.011764705882352941| 0.03561434361218788|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-14195|    1|            13| 0.07692307692307693| 0.23286301592584388|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0489|    1|            27|0.037037037037037035| 0.11211922989022112|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-2832|    1|            83|0.012048192771084338| 0.03647252056669844|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-14842|    1|            82|0.012195121951219513|0.036917307402877686|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-14822|    1|            74|0.013513513513513514| 0.04090836766264825|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-2825|    1|            83|0.012048192771084338| 0.03647252056669844|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-2808|    1|            83|0.012048192771084338| 0.03647252056669844|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-14835|    1|            80|              0.0125| 0.03784024008794963|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-14768|    1|            96|0.010416666666666666| 0.03153353340662469|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0365|    1|            24|0.041666666666666664| 0.12613413362649875|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0035|    1|            29|0.034482758620689655|  0.1043868692081369|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-35728|    1|            15| 0.06666666666666667|   0.201814613802398|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-2717|    1|            76|0.013157894736842105| 0.03983183167152592|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0334|    1|            26|0.038461538461538464| 0.11643150796292194|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0248|    1|            24|0.041666666666666664| 0.12613413362649875|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0127|    1|            27|0.037037037037037035| 0.11211922989022112|\n",
      "|interaction|990|3.0272192070359702| CVE-2020-0081|    1|            27|0.037037037037037035| 0.11211922989022112|\n",
      "|interaction|990|3.0272192070359702|CVE-2020-35490|    1|            11| 0.09090909090909091| 0.27520174609417913|\n",
      "+-----------+---+------------------+--------------+-----+--------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_df = tfidf(clean_tokens, doc_count)\n",
    "\n",
    "tfidf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402679d-b95e-415f-93b5-d72d01fc264b",
   "metadata": {},
   "source": [
    "Already calculated the TF-IDF scores for the documents. Now, let’s create a TF-IDF vector from these scores.\n",
    "\n",
    "Assuming you have a DataFrame named tfidf_vector with columns ‘id’, ‘token’, ‘tf_idf’, and ‘allTokensForId’, you can proceed as follows:\n",
    "\n",
    "    Group TF-IDF Scores by Document ID:\n",
    "        Group the DataFrame by the ‘id’ column and aggregate the ‘tf_idf’ values into a list for each document.\n",
    "        This will give you a list of TF-IDF scores for each document.\n",
    "\n",
    "    Create a Sparse Vector Representation:\n",
    "        Use the SparseVector class from PySpark to create a sparse vector representation for each document.\n",
    "        The vector will have dimensions equal to the total number of unique tokens (terms) in your dataset.\n",
    "        For each document, set the value at the index corresponding to the token to its corresponding TF-IDF score.\n",
    "\n",
    "    Assemble the Sparse Vectors:\n",
    "        Assemble the sparse vectors into a single column using the VectorAssembler.\n",
    "        This will give you a new DataFrame with a column containing the TF-IDF vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b7131c-23d2-4ba2-a8fb-f579cb838370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|           id|                                                                                                                                                                                                                                                                                                                                                                                                  tfidf_features|\n",
      "+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CVE-2020-0003|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26],[0.08608320339929297,0.29947844362650067,0.08804094396419412,0.25324861584761965,0.13978675696174284,0.05882996549838989,0.11295361009581159,0.059044173735722745,0.07316146061509812,0.12952837961636213,0.10449141529717552,0.11422584562022065,0.0841144975250346,0.2565871410888951,0.193394676821712,0.09123880994764724...|\n",
      "|CVE-2020-0006|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29],[0.0890358590304697,0.1230273692222493,0.17355998905352976,0.07220999903443813,0.2554374960343682,0.07509374632240087,0.21600617234061675,0.1612730142610483,0.16615493006911478,0.050178499983920793,0.05036120700988116,0.06240242228934839,0.11048008849630889,0.0678585695204572,0.0891250306946497,0.0974279271...|\n",
      "|CVE-2020-0009|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28],[0.0890358590304697,0.07342390878174988,0.38315624405155235,0.11338811804010202,0.23924041976876184,0.07509374632240087,0.21600617234061675,0.050178499983920793,0.11265026996863922,0.09634278508172164,0.05036120700988116,0.06240242228934839,0.11048008849630889,0.32685211111170215,0.0891250306946497,0.097427927...|\n",
      "|CVE-2020-0012|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.11643150796292194,0.23451124403372553,0.096015880714596,0.16701682433016382,0.09819951442160114,0.08605686095446208,0.2824696099838835,0.29156683463724764,0.13348981453945521,0.06585696301292153,0.0816031676091479,0.11654811706223425,0.0938200164702309,0.129802054089329,0.14748553304954848,0.10176636494160655,0.1273026367...|\n",
      "|CVE-2020-0028|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.12108876828143882,0.11802079255640024,0.18931907561902994,0.10212749499846518,0.21203843712326748,0.2937683943832388,0.13102618771114144,0.06849124153343839,0.08486729431351382,0.12121004174472362,0.20758941171885853,0.3693114624322405,0.13250198091945595,0.2814224793387917,0.101380740267624,0.15016932575820172,0.09095451...|\n",
      "|CVE-2020-0031|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[0.10811497167985608,0.10537570763964306,0.08915760352069628,0.1674818492334289,0.09118526339148676,0.2622932092707489,0.204867107119269,0.12230578845256854,0.0757743699227802,0.10822325155778893,0.1183053401066571,0.11038389840066629,0.09051851809609285,0.1340797551412515,0.1624187853749626,0.08591782803183323,0.13184010...|\n",
      "|CVE-2020-0035|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27],[0.10174206254862089,0.08608320339929297,0.06416098387821469,0.08804094396419412,0.17776721092759631,0.25324861584761965,0.18907870637502214,0.14412863968364845,0.11968052338020123,0.059044173735722745,0.07316146061509812,0.10449141529717552,0.11422584562022065,0.0841144975250346,0.24260558563688941,0.19339467682...|\n",
      "|CVE-2020-0042|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.11643150796292194,0.23451124403372553,0.113481531304231,0.096015880714596,0.16701682433016382,0.09819951442160114,0.08605686095446208,0.2824696099838835,0.29156683463724764,0.10614106797047385,0.35510717541561587,0.13348981453945521,0.06585696301292153,0.0816031676091479,0.11654811706223425,0.0938200164702309,0.1474855330...|\n",
      "|CVE-2020-0044|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.23451124403372553,0.113481531304231,0.096015880714596,0.16701682433016382,0.09819951442160114,0.08605686095446208,0.2824696099838835,0.29156683463724764,0.10614106797047385,0.13348981453945521,0.06585696301292153,0.0816031676091479,0.11654811706223425,0.0938200164702309,0.14748553304954848,0.09748148102656154,0.1443935824...|\n",
      "|CVE-2020-0050|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],[0.10808641212644833,0.09245973698442578,0.09093110989521837,0.17368488068651886,0.09456249536894924,0.11279813552518345,0.08286956980800052,0.272007772577073,0.14038403149200812,0.06341781623466516,0.07858082806806835,0.11223152013400334,0.09034520104540752,0.124994570604539,0.09799724031413963,0.10503173724406911,0.1...|\n",
      "|CVE-2020-0052|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[0.10811497167985608,0.08915760352069628,0.1674818492334289,0.09118526339148676,0.2622932092707489,0.40351911588213585,0.3297423771716433,0.13678961353334762,0.11698766759923343,0.06115289422628427,0.0757743699227802,0.1183053401066571,0.08711858672235727,0.14197468531350824,0.09449733887434893,0.11820959129404074,0.09051...|\n",
      "|CVE-2020-0063|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.12108876828143882,0.24389169379507455,0.09985651594317985,0.1736974973033704,0.17865392908887195,0.10212749499846518,0.2937683943832388,0.2259707048939961,0.20975210056966956,0.06849124153343839,0.08486729431351382,0.12121004174472362,0.13250198091945595,0.2976410836631183,0.10583701953927081,0.13239474224932563,0.1013807...|\n",
      "|CVE-2020-0064|                                                                                                                                                                               (40664,[0,1,2,3,4,5,6,7,8,9],[0.6524736359703803,0.43424374325842596,0.30455496591799536,0.4466348227221799,0.6342414802909848,0.24056991848913306,0.3691523015647587,0.38598256512622103,0.9232786560806012,0.9232786560806012])|\n",
      "|CVE-2020-0067|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[0.10811497167985608,0.10537570763964306,0.08915760352069628,0.46526115349117064,0.13768557190583816,0.14611153586797687,0.09118526339148676,0.07990994231485764,0.13114660463537445,0.2707406321631585,0.09855956311543998,0.06093103569476096,0.12395482778663698,0.06115289422628427,0.0757743699227802,0.10822325155778893,0.08...|\n",
      "|CVE-2020-0068|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.11211922989022112,0.10927851162629651,0.09245973698442578,0.48249304806491766,0.1427850375319803,0.11065367619811105,0.09456249536894924,0.08286956980800052,0.272007772577073,0.14038403149200812,0.10220991730490073,0.06318774072049285,0.06341781623466516,0.07858082806806835,0.176338806201572,0.11223152013400334,0.09034520...|\n",
      "|CVE-2020-0073|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],[0.1043868692081369,0.05959033479809504,0.16170661305296583,0.08804094396419412,0.25324861584761965,0.2614047482954634,0.18907870637502214,0.11968052338020123,0.059044173735722745,0.14632292123019625,0.10449141529717552,0.11422584562022065,0.0841144975250346,0.11637425539043288,0.193394676821712,0.13222840894097448,0.0...|\n",
      "|CVE-2020-0076|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26],[0.1043868692081369,0.2102514601681677,0.08608320339929297,0.14973922181325033,0.08804094396419412,0.07715442706262117,0.25324861584761965,0.2614047482954634,0.11968052338020123,0.059044173735722745,0.07316146061509812,0.16080150180900252,0.10449141529717552,0.0841144975250346,0.11637425539043288,0.13222840894097448...|\n",
      "|CVE-2020-0084|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[0.096015880714596,0.18036506840523114,0.09819951442160114,0.08605686095446208,0.2824696099838835,0.3395123635653019,0.259533842731462,0.16075886733945405,0.06585696301292153,0.0816031676091479,0.12740575088409226,0.14748553304954848,0.10176636494160655,0.2802644773750269,0.17629228805389027,0.21181127436805233,0.12730263...|\n",
      "|CVE-2020-0091|                                                                                                                                     (40664,[0,1,2,3,4,5,6,7,8,9,10,11],[0.5019027969002925,0.33403364866032764,0.545593876716134,0.304796492134424,0.27829502306587905,0.18505378345317927,0.2839633088959682,0.49938050283698554,0.7102143508312317,0.7102143508312317,1.4204287016624635,0.7102143508312317])|\n",
      "|CVE-2020-0092|(40664,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23],[0.11643150796292194,0.226963062608462,0.096015880714596,0.18036506840523114,0.09819951442160114,0.2824696099838835,0.152299500086241,0.35510717541561587,0.1473118914974513,0.12598671895302063,0.06585696301292153,0.0816031676091479,0.22365614912279413,0.11654811706223425,0.12740575088409226,0.0938200164702309,0.1188749675084...|\n",
      "+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql.functions import collect_list\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "grouped_tfidf = tfidf_df.groupBy('id').agg(collect_list('tf_idf').alias('tfidf_list'))\n",
    "\n",
    "# Create Sparse Vectors\n",
    "def create_sparse_vector(tfidf_list, num_tokens):\n",
    "    indices = range(num_tokens)\n",
    "    values = [0.0] * num_tokens  # Initialize all values to 0.0\n",
    "    for i, tfidf_score in enumerate(tfidf_list):\n",
    "        values[i] = tfidf_score\n",
    "    return SparseVector(num_tokens, indices, values)\n",
    "\n",
    "num_tokens = len(tfidf_df.select('token').distinct().collect())\n",
    "sparse_vector_udf = udf(create_sparse_vector, VectorUDT())\n",
    "\n",
    "tfidf_vector_with_sparse = grouped_tfidf.withColumn('tfidf_vector', sparse_vector_udf('tfidf_list', lit(num_tokens)))\n",
    "\n",
    "# Assemble the Sparse Vectors\n",
    "assembler = VectorAssembler(inputCols=['tfidf_vector'], outputCol='tfidf_features')\n",
    "final_tfidf_vector = assembler.transform(tfidf_vector_with_sparse)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "final_tfidf_vector.select('id', 'tfidf_features').show(truncate= 400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaf3fd-3797-4f44-9043-bb762e1ac213",
   "metadata": {},
   "source": [
    "Now that you have the TF-IDF vectors for the documents, here are some useful things to do with the results:\n",
    "\n",
    "    Document Similarity:\n",
    "        Calculate the similarity between documents using cosine similarity or other distance metrics. The closer the vectors, the more similar the documents.\n",
    "        For example, you can find similar documents to a given query document by comparing their TF-IDF vectors.\n",
    "\n",
    "    Topic Modeling:\n",
    "        Apply topic modeling techniques (such as Latent Dirichlet Allocation or Non-Negative Matrix Factorization) to discover underlying topics in your corpus.\n",
    "        Use the TF-IDF vectors as input for these models.\n",
    "\n",
    "    Classification and Clustering:\n",
    "        Train machine learning models (e.g., SVM, Random Forest, or k-means) using the TF-IDF vectors as features.\n",
    "        Classify documents into predefined categories or cluster similar documents together.\n",
    "\n",
    "    Keyword Extraction:\n",
    "        Identify important keywords or phrases within each document based on their TF-IDF scores.\n",
    "        Higher TF-IDF scores indicate more significant terms.\n",
    "\n",
    "    Search and Retrieval:\n",
    "        Use the TF-IDF vectors to build an efficient search index for your documents.\n",
    "        Given a query, retrieve relevant documents based on their similarity to the query.\n",
    "\n",
    "    Visualizations:\n",
    "        Visualize the TF-IDF vectors in lower dimensions using techniques like t-SNE or PCA.\n",
    "        Explore the distribution of documents in the vector space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e097ca7a-c628-4d7f-a655-76d14c28d30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 09:46:57 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "[Stage 125:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|            id|cosine_sim|\n",
      "+--------------+----------+\n",
      "| CVE-2020-0003|       1.0|\n",
      "|CVE-2020-25162| 0.9243194|\n",
      "|CVE-2020-24721|0.92241603|\n",
      "| CVE-2020-4129|0.91548777|\n",
      "| CVE-2020-8916| 0.9151929|\n",
      "+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Normalize the vectors\n",
    "normalizer = Normalizer(inputCol=\"tfidf_features\", outputCol=\"normFeatures\")\n",
    "data = normalizer.transform(final_tfidf_vector)\n",
    "\n",
    "# Get the first document's features\n",
    "first_doc_features = data.first().normFeatures\n",
    "\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "# def cosine_similarity(v):\n",
    "#     return float(first_doc_features.dot(v)) / (first_doc_features.norm(2) * v.norm(2))\n",
    "\n",
    "# Define a UDF to compute cosine similarity\n",
    "# def cosine_similarity(v):\n",
    "#     dot_product = float(first_doc_features.dot(v))\n",
    "#     norm_product = math.sqrt(sum([i**2 for i in first_doc_features])) * math.sqrt(sum([i**2 for i in v]))\n",
    "#     return dot_product / norm_product\n",
    "\n",
    "# def cosine_similarity(v):\n",
    "#     dot_product = float(first_doc_features.dot(v))\n",
    "#     norm_product = first_doc_features.norm(2) * v.norm(2)\n",
    "#     return dot_product / norm_product\n",
    "\n",
    "def cosine_similarity(v):\n",
    "    dot_product = float(first_doc_features.dot(v))\n",
    "    norm_product = float(first_doc_features.norm(2)) * float(v.norm(2))\n",
    "    return float(dot_product) / float(norm_product)\n",
    "\n",
    "\n",
    "cosine_similarity_udf = udf(cosine_similarity, FloatType())\n",
    "\n",
    "\n",
    "# Compute the cosine similarity and add it as a new column\n",
    "data = data.withColumn(\"cosine_sim\", cosine_similarity_udf(col(\"normFeatures\")))\n",
    "\n",
    "# Show the top 5 documents\n",
    "data.sort(col(\"cosine_sim\").desc()).select('id', 'cosine_sim').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f64e0d-afdb-464d-a680-97665b63017b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 08:17:32 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "[Stage 93:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|            id|cosine_sim|\n",
      "+--------------+----------+\n",
      "| CVE-2020-0003|       1.0|\n",
      "|CVE-2020-25162| 0.9243194|\n",
      "|CVE-2020-24721|0.92241603|\n",
      "| CVE-2020-4129|0.91548777|\n",
      "| CVE-2020-8916| 0.9151929|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "data.orderBy(col('cosine_sim').desc()).select('id', 'cosine_sim').limit(N).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80254d-b90f-4df1-95da-41f423b18bdb",
   "metadata": {},
   "source": [
    "# The results aren't very good at all\n",
    "\n",
    "## CVE-2020-0003 (Search Document)\n",
    "In onCreate of InstallStart.java, there is a possible package validation bypass due to a time-of-check time-of-use vulnerability. This could lead to local escalation of privilege with no additional execution privileges needed. User interaction is needed for exploitation. Product: Android Versions: Android-8.0 Android ID: A-140195904\n",
    "\n",
    "## CVE-2020-25162\n",
    "A XPath injection vulnerability in the B. Braun Melsungen AG SpaceCom Version L81/U61 and earlier, and the Data module compactplus Versions A10 and A11 allows unauthenticated remote attackers to access sensitive information and escalate privileges\n",
    "\n",
    "## CVE-2020-24721\n",
    "\"An issue was discovered in the GAEN (aka Google/Apple Exposure Notifications) protocol through 2020-09-29, as used in COVID-19 applications on Android and iOS. It allows a user to be put in a position where he or she can be coerced into proving or disproving an exposure notification, because of the persistent state of a private framework.\n",
    "\n",
    "There could be a few reasons why the documents getting aren't as similar as I'd like. Here are some things to consider:\n",
    "\n",
    "1. **Quality of your data**: The quality and relevance of the documents in your dataset can greatly affect the results of cosine similarity. If the documents in your dataset are not very similar to your target document to begin with, the top results might not seem very similar.\n",
    "\n",
    "2. **Preprocessing**: How you preprocess your data can also affect the results. This includes things like removing stop words, stemming or lemmatization, and how you handle punctuation and capitalization. You might need to adjust your preprocessing steps to better suit your data.\n",
    "\n",
    "3. **Vectorization**: The method you use to convert your text data into numerical vectors can also have a big impact. You're currently using TF-IDF, which is a good choice for many applications, but there might be other methods that work better for your specific dataset. You could experiment with other methods like Word2Vec, Doc2Vec, or BERT embeddings.\n",
    "\n",
    "4. **Dimensionality**: High-dimensional data can be problematic for cosine similarity due to the curse of dimensionality. You might want to try reducing the dimensionality of your data with techniques like PCA or t-SNE.\n",
    "\n",
    "5. **Thresholding**: You might want to consider applying a threshold to your cosine similarity scores. This means that you only consider documents as \"similar\" if their cosine similarity score is above a certain threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102ee23-5906-4781-95a5-f514033ecd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f82bcc-458e-4599-9b31-bb781f28bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a829a-d5c9-4d6a-be16-8903b6521971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
